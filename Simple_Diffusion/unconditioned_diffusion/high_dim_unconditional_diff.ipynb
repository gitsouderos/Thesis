{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import forward_diffusion_sample\n",
    "import torch\n",
    "from diffusion import get_time_embedding\n",
    "from diffusion import model_architecture\n",
    "from diffusion import reverse_diffusion_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward diffusion test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_diffusion_steps = 1000\n",
    "betas = torch.linspace(0.0001, 0.02, num_diffusion_steps)\n",
    "\n",
    "batch_size =  4\n",
    "dim = 10\n",
    "\n",
    "x_0 = torch.randn(batch_size, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 50\n",
    "timestep_tensor = torch.full((batch_size,), timestep, dtype=torch.long) # Tensor shaped (batch_size,) with timestep repeated batch_size times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t,noise = forward_diffusion_sample(x_0, timestep_tensor,betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean sample (x_0):\n",
      "tensor([[ 1.4750, -0.3938, -0.4552, -1.6739,  0.8467,  1.0316,  0.9094, -0.4057,\n",
      "         -0.9085, -0.4406],\n",
      "        [ 0.8871,  1.2570, -0.2101, -1.3765,  0.6372,  0.5948, -1.3427,  2.4970,\n",
      "          1.5104,  1.5017],\n",
      "        [ 0.2432,  1.4318, -0.5506,  0.4336,  0.2242, -0.9029,  0.6744,  1.0569,\n",
      "          1.0807,  1.3416],\n",
      "        [ 0.1433,  0.9565, -0.8070,  0.1681, -0.4470,  0.6217,  1.5089,  0.6798,\n",
      "          0.2231, -0.0413]])\n",
      "\n",
      "Noisy sample (x_t):\n",
      "tensor([[ 1.2194, -0.4151, -0.7382, -1.3455,  0.5508,  0.7636,  1.1034, -0.2733,\n",
      "         -1.0079, -0.5609],\n",
      "        [ 0.6060,  1.1153, -0.0248, -1.4095,  0.5671,  0.7894, -1.1267,  2.5681,\n",
      "          1.2953,  1.4257],\n",
      "        [ 0.2092,  1.3063, -0.6171,  0.4502,  0.4828, -0.9745,  0.6418,  1.2699,\n",
      "          0.7880,  1.5301],\n",
      "        [ 0.3794,  1.2219, -0.6230,  0.1375, -0.7460,  0.7359,  1.1333,  0.6272,\n",
      "          0.1124, -0.1248]])\n",
      "\n",
      "Noise added:\n",
      "tensor([[-1.3452, -0.1572, -1.6727,  1.7481, -1.6329, -1.4560,  1.1982,  0.7279,\n",
      "         -0.6524, -0.7324],\n",
      "        [-1.5443, -0.7077,  1.0506, -0.3105, -0.3487,  1.1748,  1.1290,  0.6281,\n",
      "         -1.1086, -0.3070],\n",
      "        [-0.1747, -0.5986, -0.4313,  0.1338,  1.5110, -0.4920, -0.1291,  1.3209,\n",
      "         -1.5943,  1.2043],\n",
      "        [ 1.3743,  1.6142,  0.9907, -0.1618, -1.7635,  0.7135, -2.0350, -0.2438,\n",
      "         -0.6194, -0.4852]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Clean sample (x_0):\")\n",
    "print(x_0)\n",
    "print(\"\\nNoisy sample (x_t):\")\n",
    "print(x_t)\n",
    "print(\"\\nNoise added:\")\n",
    "print(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time embedding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_time_embedding(timestep_tensor, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2624,  0.1566, -0.1032,  0.5084, -0.9589,  0.3239,  0.9999,  0.7765,\n",
      "          0.4794,  0.2775,  0.1575,  0.0888,  0.0500,  0.0281,  0.0158,  0.0089,\n",
      "          0.9650, -0.9877, -0.9947, -0.8611,  0.2837, -0.9461, -0.0103,  0.6301,\n",
      "          0.8776,  0.9607,  0.9875,  0.9960,  0.9988,  0.9996,  0.9999,  1.0000],\n",
      "        [-0.2624,  0.1566, -0.1032,  0.5084, -0.9589,  0.3239,  0.9999,  0.7765,\n",
      "          0.4794,  0.2775,  0.1575,  0.0888,  0.0500,  0.0281,  0.0158,  0.0089,\n",
      "          0.9650, -0.9877, -0.9947, -0.8611,  0.2837, -0.9461, -0.0103,  0.6301,\n",
      "          0.8776,  0.9607,  0.9875,  0.9960,  0.9988,  0.9996,  0.9999,  1.0000],\n",
      "        [-0.2624,  0.1566, -0.1032,  0.5084, -0.9589,  0.3239,  0.9999,  0.7765,\n",
      "          0.4794,  0.2775,  0.1575,  0.0888,  0.0500,  0.0281,  0.0158,  0.0089,\n",
      "          0.9650, -0.9877, -0.9947, -0.8611,  0.2837, -0.9461, -0.0103,  0.6301,\n",
      "          0.8776,  0.9607,  0.9875,  0.9960,  0.9988,  0.9996,  0.9999,  1.0000],\n",
      "        [-0.2624,  0.1566, -0.1032,  0.5084, -0.9589,  0.3239,  0.9999,  0.7765,\n",
      "          0.4794,  0.2775,  0.1575,  0.0888,  0.0500,  0.0281,  0.0158,  0.0089,\n",
      "          0.9650, -0.9877, -0.9947, -0.8611,  0.2837, -0.9461, -0.0103,  0.6301,\n",
      "          0.8776,  0.9607,  0.9875,  0.9960,  0.9988,  0.9996,  0.9999,  1.0000]])\n",
      "torch.Size([4, 32])\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_T shape: torch.Size([4, 10])\n",
      "time_embedding shape: torch.Size([4, 2])\n",
      "x shape: torch.Size([4, 12])\n"
     ]
    }
   ],
   "source": [
    "x_t_minus_1 = reverse_diffusion_sample(x_t, betas, timestep_tensor, model_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original sample (x_t): tensor([[ 0.9129,  0.9456,  1.9236,  0.6603,  0.0957, -0.8940,  1.2359,  1.8455,\n",
      "          2.4707, -0.1314],\n",
      "        [ 0.4904,  0.2617, -0.1398, -0.3541,  0.0190,  0.6403,  1.2285,  0.5831,\n",
      "          0.2228,  0.3880],\n",
      "        [-0.7446, -0.3469, -1.2333,  0.9391, -1.0112,  1.4118,  0.7305,  0.3503,\n",
      "         -0.8563,  0.0370],\n",
      "        [ 1.6883,  1.1925, -0.6067, -0.5078, -0.9327, -0.2396,  1.2374, -0.2426,\n",
      "         -0.5686, -0.3173]])\n",
      " Noise added: tensor([[-0.5921,  0.8497,  1.6738,  0.4271, -1.4904, -0.8752, -1.5927, -0.7021,\n",
      "          0.9303,  0.0672],\n",
      "        [-0.7069,  0.5148, -0.2175,  0.2886,  0.1850,  0.9603, -0.7536, -0.7710,\n",
      "          1.1935,  1.2176],\n",
      "        [ 1.0512, -1.2152,  1.1807,  0.4453, -0.5517,  1.9664,  0.4839, -0.1953,\n",
      "          0.0798, -0.0605],\n",
      "        [ 0.0398,  0.3673,  0.4900, -0.7888, -0.4366,  0.5434, -1.3997,  0.6173,\n",
      "         -1.0215, -1.7815]])\n",
      " Reversed sample (x_t-1): tensor([[ 0.9119,  0.9473,  1.9252,  0.6607,  0.0961, -0.8930,  1.2362,  1.8470,\n",
      "          2.4713, -0.1303],\n",
      "        [ 0.4905,  0.2621, -0.1396, -0.3538,  0.0193,  0.6413,  1.2285,  0.5840,\n",
      "          0.2227,  0.3890],\n",
      "        [-0.7446, -0.3468, -1.2338,  0.9402, -1.0112,  1.4118,  0.7300,  0.3505,\n",
      "         -0.8563,  0.0376],\n",
      "        [ 1.6890,  1.1934, -0.6066, -0.5074, -0.9330, -0.2394,  1.2371, -0.2425,\n",
      "         -0.5695, -0.3162]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\" Original sample (x_t): {x_t}\")\n",
    "print(f\" Noise added: {noise}\")\n",
    "print(f\" Reversed sample (x_t-1): {x_t_minus_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from diffusion import run_reverse_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../Datasets'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from multi_target_dataset import StockDiffusionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258\n"
     ]
    }
   ],
   "source": [
    "base_path = \"../../price/raw\"\n",
    "df = pd.read_csv(f\"{base_path}/AAPL.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Daily Return (percentage change from open to close)\n",
    "df['Return'] = (df['Close'] - df['Open']) / df['Open']\n",
    "\n",
    "# 2. Price Difference (Close - Open)\n",
    "df['Diff'] = df['Close'] - df['Open']\n",
    "\n",
    "# 3. High-Low Difference (as a measure of volatility)\n",
    "df['HL_Diff'] = df['High'] - df['Low']\n",
    "\n",
    "# 4. 5-day Moving Average of the Close (shifted to avoid leakage)\n",
    "df['MA5'] = df['Close'].rolling(window=5).mean().shift(1)\n",
    "\n",
    "# 5. 5-day Moving Average of the Return (shifted)\n",
    "df['Return_MA5'] = df['Return'].rolling(window=5).mean().shift(1)\n",
    "\n",
    "# Drop rows with NaN values that result from rolling and shifting\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Return</th>\n",
       "      <th>Diff</th>\n",
       "      <th>HL_Diff</th>\n",
       "      <th>MA5</th>\n",
       "      <th>Return_MA5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-09-11</td>\n",
       "      <td>95.015717</td>\n",
       "      <td>95.728569</td>\n",
       "      <td>93.785713</td>\n",
       "      <td>94.370003</td>\n",
       "      <td>85.265068</td>\n",
       "      <td>125995800</td>\n",
       "      <td>-0.006796</td>\n",
       "      <td>-0.645714</td>\n",
       "      <td>1.942856</td>\n",
       "      <td>96.132857</td>\n",
       "      <td>-0.002394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-09-12</td>\n",
       "      <td>95.264282</td>\n",
       "      <td>95.699997</td>\n",
       "      <td>93.714287</td>\n",
       "      <td>95.684288</td>\n",
       "      <td>86.452538</td>\n",
       "      <td>178058300</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>0.420006</td>\n",
       "      <td>1.985710</td>\n",
       "      <td>95.722000</td>\n",
       "      <td>-0.006519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-09-13</td>\n",
       "      <td>96.767143</td>\n",
       "      <td>97.928574</td>\n",
       "      <td>96.395714</td>\n",
       "      <td>97.568573</td>\n",
       "      <td>88.155037</td>\n",
       "      <td>149590000</td>\n",
       "      <td>0.008282</td>\n",
       "      <td>0.801430</td>\n",
       "      <td>1.532860</td>\n",
       "      <td>95.709428</td>\n",
       "      <td>-0.004057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-09-14</td>\n",
       "      <td>98.565712</td>\n",
       "      <td>99.568573</td>\n",
       "      <td>98.269997</td>\n",
       "      <td>98.754288</td>\n",
       "      <td>89.226341</td>\n",
       "      <td>150118500</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.188576</td>\n",
       "      <td>1.298576</td>\n",
       "      <td>95.901143</td>\n",
       "      <td>-0.003321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-09-17</td>\n",
       "      <td>99.907143</td>\n",
       "      <td>99.971428</td>\n",
       "      <td>99.230003</td>\n",
       "      <td>99.968575</td>\n",
       "      <td>90.323479</td>\n",
       "      <td>99507800</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.061432</td>\n",
       "      <td>0.741425</td>\n",
       "      <td>96.210858</td>\n",
       "      <td>-0.003644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close  \\\n",
       "0  2012-09-11  95.015717  95.728569  93.785713  94.370003  85.265068   \n",
       "1  2012-09-12  95.264282  95.699997  93.714287  95.684288  86.452538   \n",
       "2  2012-09-13  96.767143  97.928574  96.395714  97.568573  88.155037   \n",
       "3  2012-09-14  98.565712  99.568573  98.269997  98.754288  89.226341   \n",
       "4  2012-09-17  99.907143  99.971428  99.230003  99.968575  90.323479   \n",
       "\n",
       "      Volume    Return      Diff   HL_Diff        MA5  Return_MA5  \n",
       "0  125995800 -0.006796 -0.645714  1.942856  96.132857   -0.002394  \n",
       "1  178058300  0.004409  0.420006  1.985710  95.722000   -0.006519  \n",
       "2  149590000  0.008282  0.801430  1.532860  95.709428   -0.004057  \n",
       "3  150118500  0.001913  0.188576  1.298576  95.901143   -0.003321  \n",
       "4   99507800  0.000615  0.061432  0.741425  96.210858   -0.003644  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['Open', 'High', 'Low', 'Volume', 'Return', 'Diff', 'HL_Diff', 'MA5', 'Return_MA5']\n",
    "# We keep the target as the raw 'Close' (normalized later)\n",
    "target_column = 'Close'\n",
    "\n",
    "filtered_df = df[feature_columns + [target_column]]\n",
    "\n",
    "seq_len = 10\n",
    "\n",
    "# Split into training and test data \n",
    "train_size = int(len(filtered_df) * 0.8)\n",
    "train_data = filtered_df.iloc[:train_size].copy()\n",
    "test_data = filtered_df.iloc[train_size:].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Return</th>\n",
       "      <th>Diff</th>\n",
       "      <th>HL_Diff</th>\n",
       "      <th>MA5</th>\n",
       "      <th>Return_MA5</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95.015717</td>\n",
       "      <td>95.728569</td>\n",
       "      <td>93.785713</td>\n",
       "      <td>125995800</td>\n",
       "      <td>-0.006796</td>\n",
       "      <td>-0.645714</td>\n",
       "      <td>1.942856</td>\n",
       "      <td>96.132857</td>\n",
       "      <td>-0.002394</td>\n",
       "      <td>94.370003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.264282</td>\n",
       "      <td>95.699997</td>\n",
       "      <td>93.714287</td>\n",
       "      <td>178058300</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>0.420006</td>\n",
       "      <td>1.985710</td>\n",
       "      <td>95.722000</td>\n",
       "      <td>-0.006519</td>\n",
       "      <td>95.684288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96.767143</td>\n",
       "      <td>97.928574</td>\n",
       "      <td>96.395714</td>\n",
       "      <td>149590000</td>\n",
       "      <td>0.008282</td>\n",
       "      <td>0.801430</td>\n",
       "      <td>1.532860</td>\n",
       "      <td>95.709428</td>\n",
       "      <td>-0.004057</td>\n",
       "      <td>97.568573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98.565712</td>\n",
       "      <td>99.568573</td>\n",
       "      <td>98.269997</td>\n",
       "      <td>150118500</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.188576</td>\n",
       "      <td>1.298576</td>\n",
       "      <td>95.901143</td>\n",
       "      <td>-0.003321</td>\n",
       "      <td>98.754288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.907143</td>\n",
       "      <td>99.971428</td>\n",
       "      <td>99.230003</td>\n",
       "      <td>99507800</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.061432</td>\n",
       "      <td>0.741425</td>\n",
       "      <td>96.210858</td>\n",
       "      <td>-0.003644</td>\n",
       "      <td>99.968575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>106.620003</td>\n",
       "      <td>107.440002</td>\n",
       "      <td>106.290001</td>\n",
       "      <td>24970300</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.199997</td>\n",
       "      <td>1.150001</td>\n",
       "      <td>107.980000</td>\n",
       "      <td>-0.001699</td>\n",
       "      <td>106.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>105.800003</td>\n",
       "      <td>106.500000</td>\n",
       "      <td>105.500000</td>\n",
       "      <td>24863900</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.199997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>107.642000</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>105.660004</td>\n",
       "      <td>106.570000</td>\n",
       "      <td>105.639999</td>\n",
       "      <td>29662400</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.439994</td>\n",
       "      <td>0.930001</td>\n",
       "      <td>107.072000</td>\n",
       "      <td>-0.000781</td>\n",
       "      <td>106.099998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>106.139999</td>\n",
       "      <td>106.800003</td>\n",
       "      <td>105.620003</td>\n",
       "      <td>26701500</td>\n",
       "      <td>0.005559</td>\n",
       "      <td>0.590004</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>106.686000</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>106.730003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>107.699997</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>106.820000</td>\n",
       "      <td>26802500</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.030006</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>106.518001</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>107.730003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1002 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open        High         Low     Volume    Return      Diff  \\\n",
       "0      95.015717   95.728569   93.785713  125995800 -0.006796 -0.645714   \n",
       "1      95.264282   95.699997   93.714287  178058300  0.004409  0.420006   \n",
       "2      96.767143   97.928574   96.395714  149590000  0.008282  0.801430   \n",
       "3      98.565712   99.568573   98.269997  150118500  0.001913  0.188576   \n",
       "4      99.907143   99.971428   99.230003   99507800  0.000615  0.061432   \n",
       "...          ...         ...         ...        ...       ...       ...   \n",
       "997   106.620003  107.440002  106.290001   24970300  0.001876  0.199997   \n",
       "998   105.800003  106.500000  105.500000   24863900  0.001890  0.199997   \n",
       "999   105.660004  106.570000  105.639999   29662400  0.004164  0.439994   \n",
       "1000  106.139999  106.800003  105.620003   26701500  0.005559  0.590004   \n",
       "1001  107.699997  108.000000  106.820000   26802500  0.000279  0.030006   \n",
       "\n",
       "       HL_Diff         MA5  Return_MA5       Close  \n",
       "0     1.942856   96.132857   -0.002394   94.370003  \n",
       "1     1.985710   95.722000   -0.006519   95.684288  \n",
       "2     1.532860   95.709428   -0.004057   97.568573  \n",
       "3     1.298576   95.901143   -0.003321   98.754288  \n",
       "4     0.741425   96.210858   -0.003644   99.968575  \n",
       "...        ...         ...         ...         ...  \n",
       "997   1.150001  107.980000   -0.001699  106.820000  \n",
       "998   1.000000  107.642000   -0.000681  106.000000  \n",
       "999   0.930001  107.072000   -0.000781  106.099998  \n",
       "1000  1.180000  106.686000    0.001046  106.730003  \n",
       "1001  1.180000  106.518001    0.001823  107.730003  \n",
       "\n",
       "[1002 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_822214/3772158868.py:5: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.32072709 0.46856191 0.38772434 ... 0.0471822  0.03877453 0.03906133]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train_data.loc[:, train_data.columns] = scaler.transform(train_data)\n",
      "/tmp/ipykernel_822214/3772158868.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.03928253  0.08325006  0.1134565   0.0951555   0.09156573  0.13950723\n",
      "  0.27782951  0.21846813  0.1897979   0.09647874  0.06095948  0.06518731\n",
      "  0.05119053  0.11197765  0.04776999  0.0328282   0.04712172  0.06485735\n",
      "  0.0662547   0.02457757  0.04739346  0.02387137  0.04467458  0.03212115\n",
      "  0.06584836  0.14480245  0.06968404  0.06288499  0.06419062  0.03003833\n",
      "  0.03267515  0.01984344  0.03146067  0.02881107  0.02979356  0.0996193\n",
      "  0.15074622  0.06109493  0.07046464  0.03797349  0.08740009  0.04340359\n",
      "  0.03943076  0.05051755  0.05541012  0.03125821  0.1309891   0.125191\n",
      "  0.0597663   0.10827003  0.05457103  0.13003529  0.04141675  0.0436796\n",
      "  0.04605262  0.03668462  0.04083294 -0.00445954  0.04017302  0.04396327\n",
      "  0.06563909  0.06826455  0.03828187  0.06042053  0.03733772  0.04813715\n",
      "  0.03981609  0.0606423   0.03784571  0.08713885  0.05958939  0.09506322\n",
      "  0.08889171  0.0418353   0.02379157  0.03048783  0.0370265   0.00322234\n",
      "  0.01490913  0.02231756  0.00565954  0.04980567  0.04468196  0.02292011\n",
      "  0.02597406  0.05311547  0.05825508  0.03241561  0.04129351  0.03986692\n",
      "  0.03710033  0.06074794  0.0302885   0.03563909  0.05551774  0.02556687\n",
      "  0.02886304  0.05489219  0.03774122  0.02134359  0.04921277  0.10266331\n",
      "  0.28094252  0.05867676  0.03254396  0.03918457  0.07137926  0.02827553\n",
      "  0.04345527  0.01993119  0.02836441  0.05730184  0.06410799  0.02708433\n",
      "  0.02598712  0.03254368  0.02212163  0.02198334  0.02478996  0.0204761\n",
      "  0.02963511  0.06635551  0.03738173  0.02289172  0.02471443  0.0124938\n",
      "  0.01607421  0.02586701  0.01864572  0.01242395  0.00642508  0.03590743\n",
      "  0.01756441  0.08756819  0.0241238   0.0752016   0.03638561  0.02072854\n",
      "  0.02654765  0.02989692  0.0577238   0.04584079  0.0231734   0.01878457\n",
      "  0.01970459  0.01943682  0.04166067  0.02300785  0.01029569  0.01671652\n",
      "  0.04921817  0.02073904  0.01356318  0.01003985  0.0046884   0.01215901\n",
      "  0.02917141  0.01213772  0.01160786  0.01654075  0.01986218  0.00340719\n",
      "  0.02218836  0.0583715   0.0917344   0.09271348  0.02931992  0.04055267\n",
      "  0.10138948  0.0740672   0.03623085  0.04034652  0.05531642  0.03681012\n",
      "  0.01988291  0.10711206  0.05827297  0.03951083  0.02816848  0.01951491\n",
      "  0.01741107  0.01757463  0.02457558  0.02010554  0.03238466  0.00953441\n",
      "  0.0418106   0.0348849   0.03855702  0.02278239  0.02329692  0.14719251\n",
      "  0.16827514  0.05996876  0.05248878  0.05428963  0.10595749  0.05535731\n",
      "  0.03365934  0.02333952  0.01720748  0.06358636  0.03590913  0.03326692\n",
      "  0.0256583   0.05239848  0.02833232  0.00344127  0.02420217  0.03146919\n",
      "  0.01747837  0.02284202  0.0191256   0.03361505  0.03450923  0.0201203\n",
      "  0.03051708  0.01369352  0.02236611  0.0119185   0.03749985  0.02398523\n",
      "  0.01649077  0.00776507  0.05517245  0.01183332  0.01930762  0.06338532\n",
      "  0.16154395  0.03989843  0.02133507  0.02505603  0.06576289  0.03715598\n",
      "  0.07882034  0.03751263  0.0255044   0.04662309  0.0415292   0.04229304\n",
      "  0.04083776  0.03782896  0.02430156  0.0180389   0.01923095  0.03530629\n",
      "  0.03668604  0.04676904  0.04038769  0.03901192  0.00995665]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  test_data.loc[:, test_data.columns] = scaler.transform(test_data)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "\n",
    "# Transform train and test sets.\n",
    "train_data.loc[:, train_data.columns] = scaler.transform(train_data)\n",
    "test_data.loc[:, test_data.columns] = scaler.transform(test_data)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "target_column= ['Open', 'High', 'Low', 'Volume', 'Return', 'Diff', 'HL_Diff', 'MA5', 'Return_MA5']\n",
    "\n",
    "train_dataset = StockDiffusionDataset(train_data, target_column=target_column)\n",
    "test_dataset = StockDiffusionDataset(test_data, target_column=target_column)\n",
    "print(\"Datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5041, 0.4985, 0.5066, 0.4686, 0.4615, 0.4987, 0.0934, 0.5208, 0.4064])\n"
     ]
    }
   ],
   "source": [
    "#Visualize the first sample\n",
    "sample = train_dataset[1]\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6639, 0.6612, 0.6872, 0.0393, 0.4206, 0.4590, 0.0202, 0.6676, 0.6465])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 9])\n"
     ]
    }
   ],
   "source": [
    "for x0 in train_loader :\n",
    "    # print(x0)\n",
    "    print(x0.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import model_architecture\n",
    "from diffusion import forward_diffusion_sample\n",
    "from diffusion import reverse_diffusion_sample\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from diffusion import ResidualMLP\n",
    "from diffusion import cosine_beta_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.16372939944267273\n",
      "Epoch 2, Loss: 19.219627380371094\n",
      "Epoch 3, Loss: 55.393009185791016\n",
      "Epoch 4, Loss: 0.3788753151893616\n",
      "Epoch 5, Loss: 0.26734745502471924\n",
      "Epoch 6, Loss: 0.16249114274978638\n",
      "Epoch 7, Loss: 0.2294081598520279\n",
      "Epoch 8, Loss: 0.1243773028254509\n",
      "Epoch 9, Loss: 38.24098205566406\n",
      "Epoch 10, Loss: 0.08309700340032578\n",
      "Epoch 11, Loss: 0.04958854615688324\n",
      "Epoch 12, Loss: 0.16039162874221802\n",
      "Epoch 13, Loss: 31.60200309753418\n",
      "Epoch 14, Loss: 0.06078803539276123\n",
      "Epoch 15, Loss: 0.2627975642681122\n",
      "Epoch 16, Loss: 0.11299579590559006\n",
      "Epoch 17, Loss: 0.07903236150741577\n",
      "Epoch 18, Loss: 0.05871172249317169\n",
      "Epoch 19, Loss: 0.07022206485271454\n",
      "Epoch 20, Loss: 0.11240698397159576\n",
      "Epoch 21, Loss: 0.26558762788772583\n",
      "Epoch 22, Loss: 0.04356837272644043\n",
      "Epoch 23, Loss: 0.10445287823677063\n",
      "Epoch 24, Loss: 0.13824191689491272\n",
      "Epoch 25, Loss: 0.10941196978092194\n",
      "Epoch 26, Loss: 0.12296038120985031\n",
      "Epoch 27, Loss: 0.11616577208042145\n",
      "Epoch 28, Loss: 0.14335210621356964\n",
      "Epoch 29, Loss: 0.14944039285182953\n",
      "Epoch 30, Loss: 0.15187294781208038\n",
      "Epoch 31, Loss: 0.1315327137708664\n",
      "Epoch 32, Loss: 0.08236449956893921\n",
      "Epoch 33, Loss: 0.14549891650676727\n",
      "Epoch 34, Loss: 0.22220784425735474\n",
      "Epoch 35, Loss: 0.06236867234110832\n",
      "Epoch 36, Loss: 0.0719335600733757\n",
      "Epoch 37, Loss: 0.10781559348106384\n",
      "Epoch 38, Loss: 0.1067771315574646\n",
      "Epoch 39, Loss: 0.0966731607913971\n",
      "Epoch 40, Loss: 0.10723067820072174\n",
      "Epoch 41, Loss: 0.13060146570205688\n",
      "Epoch 42, Loss: 0.15195982158184052\n",
      "Epoch 43, Loss: 0.16834254562854767\n",
      "Epoch 44, Loss: 0.09306357800960541\n",
      "Epoch 45, Loss: 0.15441608428955078\n",
      "Epoch 46, Loss: 0.08525027334690094\n",
      "Epoch 47, Loss: 33.67951965332031\n",
      "Epoch 48, Loss: 0.08923743665218353\n",
      "Epoch 49, Loss: 0.12705858051776886\n",
      "Epoch 50, Loss: 0.1947878897190094\n",
      "Epoch 51, Loss: 0.047232791781425476\n",
      "Epoch 52, Loss: 0.08549007773399353\n",
      "Epoch 53, Loss: 0.046426739543676376\n",
      "Epoch 54, Loss: 0.08968266099691391\n",
      "Epoch 55, Loss: 0.15419580042362213\n",
      "Epoch 56, Loss: 0.060053400695323944\n",
      "Epoch 57, Loss: 0.13260196149349213\n",
      "Epoch 58, Loss: 0.05380546674132347\n",
      "Epoch 59, Loss: 0.08846703171730042\n",
      "Epoch 60, Loss: 0.08096180111169815\n",
      "Epoch 61, Loss: 0.15850196778774261\n",
      "Epoch 62, Loss: 0.058484990149736404\n",
      "Epoch 63, Loss: 0.10752533376216888\n",
      "Epoch 64, Loss: 0.07791023701429367\n",
      "Epoch 65, Loss: 14.26937484741211\n",
      "Epoch 66, Loss: 0.0900505855679512\n",
      "Epoch 67, Loss: 0.11693678796291351\n",
      "Epoch 68, Loss: 0.12510721385478973\n",
      "Epoch 69, Loss: 0.22922182083129883\n",
      "Epoch 70, Loss: 0.06270959973335266\n",
      "Epoch 71, Loss: 0.09671100974082947\n",
      "Epoch 72, Loss: 0.07132763415575027\n",
      "Epoch 73, Loss: 0.11671122908592224\n",
      "Epoch 74, Loss: 0.11961380392313004\n",
      "Epoch 75, Loss: 0.12136144191026688\n",
      "Epoch 76, Loss: 0.11401954293251038\n",
      "Epoch 77, Loss: 0.18299895524978638\n",
      "Epoch 78, Loss: 0.06686727702617645\n",
      "Epoch 79, Loss: 0.08792819827795029\n",
      "Epoch 80, Loss: 0.059467460960149765\n",
      "Epoch 81, Loss: 0.08737104386091232\n",
      "Epoch 82, Loss: 12.714112281799316\n",
      "Epoch 83, Loss: 15.799516677856445\n",
      "Epoch 84, Loss: 0.05762079730629921\n",
      "Epoch 85, Loss: 0.08054230362176895\n",
      "Epoch 86, Loss: 0.080571748316288\n",
      "Epoch 87, Loss: 31.06898307800293\n",
      "Epoch 88, Loss: 10.75136661529541\n",
      "Epoch 89, Loss: 0.06675943732261658\n",
      "Epoch 90, Loss: 0.049891844391822815\n",
      "Epoch 91, Loss: 0.10299333184957504\n",
      "Epoch 92, Loss: 0.0900927186012268\n",
      "Epoch 93, Loss: 0.052101802080869675\n",
      "Epoch 94, Loss: 0.1752333790063858\n",
      "Epoch 95, Loss: 7.931840896606445\n",
      "Epoch 96, Loss: 0.06646320223808289\n",
      "Epoch 97, Loss: 0.10937333852052689\n",
      "Epoch 98, Loss: 0.07811836898326874\n",
      "Epoch 99, Loss: 0.04338384419679642\n",
      "Epoch 100, Loss: 0.08912120759487152\n",
      "Epoch 101, Loss: 0.1248742863535881\n",
      "Epoch 102, Loss: 0.09877962619066238\n",
      "Epoch 103, Loss: 0.16969411075115204\n",
      "Epoch 104, Loss: 0.057953741401433945\n",
      "Epoch 105, Loss: 0.14012835919857025\n",
      "Epoch 106, Loss: 0.0392463281750679\n",
      "Epoch 107, Loss: 0.06440688669681549\n",
      "Epoch 108, Loss: 0.06368056684732437\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Apply gradient clipping to the model's parameters (adjust max_norm as needed)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(denoise_net\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis_env/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis_env/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis_env/lib/python3.10/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis_env/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis_env/lib/python3.10/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis_env/lib/python3.10/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 432\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_diffusion_steps = 100  # Total diffusion steps\n",
    "num_epochs = 1000\n",
    "batch_size = 16\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# Create a beta schedule: linearly spaced between 0.0001 and 0.01\n",
    "# betas = torch.linspace(0.0001, 0.01, num_diffusion_steps)\n",
    "betas = cosine_beta_schedule(num_diffusion_steps, s=0.008)\n",
    "\n",
    "\n",
    "# Since our targets are scalars, dim = 1\n",
    "dim = 9\n",
    "embedding_dim = 32  # As used in get_time_embedding\n",
    "# Instantiate the denoising network\n",
    "denoise_net = ResidualMLP(dim, embedding_dim, hidden_size=256)\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(denoise_net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x0 in train_loader:  # x0 has shape [batch_size] or [batch_size, 1]\n",
    "        # If necessary, reshape x0 to [batch_size, dim] (here dim might be 1)\n",
    "        # x0 = x0.unsqueeze(1)  if it's a scalar\n",
    "        # print(x0.shape)\n",
    "        \n",
    "        # Sample a random diffusion timestep for each sample in the batch\n",
    "        t = torch.randint(0, num_diffusion_steps, (batch_size,), dtype=torch.long)\n",
    "        # print(t.shape)\n",
    "        \n",
    "        # Generate the noisy sample and the true noise using forward diffusion\n",
    "        x_t, true_noise = forward_diffusion_sample(x0, t, betas)\n",
    "        \n",
    "        # Use the reverse diffusion function to predict the noise\n",
    "        predicted_noise = reverse_diffusion_sample(x_t, betas, t, embedding_dim, denoise_net)\n",
    "        # print(f'predicted_noise : {predicted_noise.shape}')\n",
    "        \n",
    "        # Compute the loss (MSE between predicted noise and true noise)\n",
    "        loss = F.mse_loss(predicted_noise, true_noise)\n",
    "        \n",
    "        # Backpropagate and update the network parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradient clipping to the model's parameters (adjust max_norm as needed)\n",
    "        torch.nn.utils.clip_grad_norm_(denoise_net.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}, LR: {current_lr}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 99\n",
      "timestep: 98\n",
      "timestep: 97\n",
      "timestep: 96\n",
      "timestep: 95\n",
      "timestep: 94\n",
      "timestep: 93\n",
      "timestep: 92\n",
      "timestep: 91\n",
      "timestep: 90\n",
      "timestep: 89\n",
      "timestep: 88\n",
      "timestep: 87\n",
      "timestep: 86\n",
      "timestep: 85\n",
      "timestep: 84\n",
      "timestep: 83\n",
      "timestep: 82\n",
      "timestep: 81\n",
      "timestep: 80\n",
      "timestep: 79\n",
      "timestep: 78\n",
      "timestep: 77\n",
      "timestep: 76\n",
      "timestep: 75\n",
      "timestep: 74\n",
      "timestep: 73\n",
      "timestep: 72\n",
      "timestep: 71\n",
      "timestep: 70\n",
      "timestep: 69\n",
      "timestep: 68\n",
      "timestep: 67\n",
      "timestep: 66\n",
      "timestep: 65\n",
      "timestep: 64\n",
      "timestep: 63\n",
      "timestep: 62\n",
      "timestep: 61\n",
      "timestep: 60\n",
      "timestep: 59\n",
      "timestep: 58\n",
      "timestep: 57\n",
      "timestep: 56\n",
      "timestep: 55\n",
      "timestep: 54\n",
      "timestep: 53\n",
      "timestep: 52\n",
      "timestep: 51\n",
      "timestep: 50\n",
      "timestep: 49\n",
      "timestep: 48\n",
      "timestep: 47\n",
      "timestep: 46\n",
      "timestep: 45\n",
      "timestep: 44\n",
      "timestep: 43\n",
      "timestep: 42\n",
      "timestep: 41\n",
      "timestep: 40\n",
      "timestep: 39\n",
      "timestep: 38\n",
      "timestep: 37\n",
      "timestep: 36\n",
      "timestep: 35\n",
      "timestep: 34\n",
      "timestep: 33\n",
      "timestep: 32\n",
      "timestep: 31\n",
      "timestep: 30\n",
      "timestep: 29\n",
      "timestep: 28\n",
      "timestep: 27\n",
      "timestep: 26\n",
      "timestep: 25\n",
      "timestep: 24\n",
      "timestep: 23\n",
      "timestep: 22\n",
      "timestep: 21\n",
      "timestep: 20\n",
      "timestep: 19\n",
      "timestep: 18\n",
      "timestep: 17\n",
      "timestep: 16\n",
      "timestep: 15\n",
      "timestep: 14\n",
      "timestep: 13\n",
      "timestep: 12\n",
      "timestep: 11\n",
      "timestep: 10\n",
      "timestep: 9\n",
      "timestep: 8\n",
      "timestep: 7\n",
      "timestep: 6\n",
      "timestep: 5\n",
      "timestep: 4\n",
      "timestep: 3\n",
      "timestep: 2\n",
      "timestep: 1\n",
      "timestep: 0\n",
      "Generated samples (predicted clean x0):\n",
      "tensor([[-3.6147e+03, -2.4427e+03, -3.0702e+03, -1.6323e+03, -3.1513e+03,\n",
      "         -3.7597e+03, -1.8069e+02, -3.7501e+03, -4.5202e+03],\n",
      "        [-2.4498e+03, -1.6611e+03, -2.0871e+03, -1.1066e+03, -2.1377e+03,\n",
      "         -2.5524e+03, -1.2373e+02, -2.5444e+03, -3.0701e+03],\n",
      "        [-1.8078e+05, -1.2197e+05, -1.5355e+05, -8.1491e+04, -1.5751e+05,\n",
      "         -1.8820e+05, -9.0150e+03, -1.8779e+05, -2.2629e+05],\n",
      "        [-2.7870e+05, -1.8809e+05, -2.3678e+05, -1.2566e+05, -2.4279e+05,\n",
      "         -2.9015e+05, -1.3853e+04, -2.8947e+05, -3.4890e+05],\n",
      "        [-8.6794e+04, -5.8545e+04, -7.3716e+04, -3.9145e+04, -7.5602e+04,\n",
      "         -9.0308e+04, -4.2814e+03, -9.0136e+04, -1.0862e+05],\n",
      "        [-3.7745e+03, -2.5509e+03, -3.2101e+03, -1.7031e+03, -3.2916e+03,\n",
      "         -3.9296e+03, -1.9172e+02, -3.9212e+03, -4.7266e+03],\n",
      "        [-2.1494e+07, -1.4505e+07, -1.8260e+07, -9.6895e+06, -1.8726e+07,\n",
      "         -2.2379e+07, -1.0705e+06, -2.2328e+07, -2.6910e+07],\n",
      "        [-5.8580e+05, -3.9567e+05, -4.9802e+05, -2.6418e+05, -5.1023e+05,\n",
      "         -6.1014e+05, -2.8874e+04, -6.0852e+05, -7.3372e+05],\n",
      "        [-3.4659e+04, -2.3377e+04, -2.9436e+04, -1.5629e+04, -3.0188e+04,\n",
      "         -3.6068e+04, -1.7189e+03, -3.6001e+04, -4.3376e+04],\n",
      "        [-6.6865e+04, -4.5159e+04, -5.6844e+04, -3.0142e+04, -5.8265e+04,\n",
      "         -6.9655e+04, -3.3460e+03, -6.9482e+04, -8.3762e+04],\n",
      "        [-4.5209e+03, -3.0765e+03, -3.8667e+03, -2.0393e+03, -3.9471e+03,\n",
      "         -4.7300e+03, -2.3401e+02, -4.7024e+03, -5.6849e+03],\n",
      "        [-6.6248e+03, -4.4718e+03, -5.6256e+03, -2.9877e+03, -5.7738e+03,\n",
      "         -6.8925e+03, -3.3180e+02, -6.8784e+03, -8.2885e+03],\n",
      "        [-2.7163e+06, -1.8335e+06, -2.3080e+06, -1.2246e+06, -2.3664e+06,\n",
      "         -2.8284e+06, -1.3488e+05, -2.8216e+06, -3.4010e+06],\n",
      "        [-2.1675e+06, -1.4626e+06, -1.8413e+06, -9.7713e+05, -1.8884e+06,\n",
      "         -2.2566e+06, -1.0798e+05, -2.2516e+06, -2.7135e+06],\n",
      "        [-1.0373e+06, -7.0029e+05, -8.8145e+05, -4.6768e+05, -9.0372e+05,\n",
      "         -1.0802e+06, -5.1555e+04, -1.0775e+06, -1.2989e+06],\n",
      "        [-7.1410e+05, -4.8187e+05, -6.0663e+05, -3.2194e+05, -6.2213e+05,\n",
      "         -7.4344e+05, -3.5527e+04, -7.4177e+05, -8.9399e+05]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "denoise_net.eval()\n",
    "\n",
    "# Define hyperparameters \n",
    "num_diffusion_steps = 100  \n",
    "batch_size = 16            \n",
    "dim = 9               \n",
    "\n",
    "# Run the reverse diffusion chain starting from noise\n",
    "generated_samples = run_reverse_diffusion(denoise_net, betas, num_diffusion_steps, batch_size, dim,embedding_dim)\n",
    "\n",
    "print(\"Generated samples (predicted clean x0):\")\n",
    "print(generated_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test MSE: 12751478745770.666\n",
      "Batch 1 predictions:\n",
      " tensor([[-2.4264e+04, -1.6379e+04, -2.0595e+04, -1.0937e+04, -2.1140e+04,\n",
      "         -2.5254e+04, -1.1876e+03, -2.5170e+04, -3.0341e+04],\n",
      "        [-4.2778e+02, -3.2192e+02, -3.7281e+02, -1.6098e+02, -3.8046e+02,\n",
      "         -4.8266e+02, -4.7737e+01, -4.4148e+02, -5.3946e+02],\n",
      "        [-1.0409e+03, -7.0924e+02, -8.8895e+02, -4.6992e+02, -9.1016e+02,\n",
      "         -1.0865e+03, -5.3091e+01, -1.0814e+03, -1.3051e+03],\n",
      "        [-2.4790e+03, -1.6779e+03, -2.1076e+03, -1.1213e+03, -2.1623e+03,\n",
      "         -2.5780e+03, -1.2327e+02, -2.5719e+03, -3.1011e+03],\n",
      "        [-1.9404e+03, -1.3166e+03, -1.6536e+03, -8.7697e+02, -1.6943e+03,\n",
      "         -2.0209e+03, -9.7857e+01, -2.0136e+03, -2.4305e+03],\n",
      "        [-6.3894e+05, -4.3116e+05, -5.4279e+05, -2.8805e+05, -5.5667e+05,\n",
      "         -6.6522e+05, -3.1811e+04, -6.6372e+05, -7.9990e+05],\n",
      "        [-7.6893e+02, -5.2203e+02, -6.5635e+02, -3.5105e+02, -6.6945e+02,\n",
      "         -7.9473e+02, -3.7631e+01, -7.9118e+02, -9.5824e+02],\n",
      "        [-1.1434e+05, -7.7161e+04, -9.7134e+04, -5.1551e+04, -9.9620e+04,\n",
      "         -1.1904e+05, -5.6914e+03, -1.1877e+05, -1.4314e+05],\n",
      "        [-5.3572e+04, -3.6087e+04, -4.5449e+04, -2.4166e+04, -4.6665e+04,\n",
      "         -5.5692e+04, -2.6822e+03, -5.5682e+04, -6.7020e+04],\n",
      "        [-2.1934e+03, -1.4858e+03, -1.8665e+03, -9.9100e+02, -1.9137e+03,\n",
      "         -2.2831e+03, -1.1094e+02, -2.2773e+03, -2.7466e+03],\n",
      "        [-5.3860e+04, -3.6295e+04, -4.5718e+04, -2.4296e+04, -4.6871e+04,\n",
      "         -5.6002e+04, -2.6299e+03, -5.5930e+04, -6.7383e+04],\n",
      "        [-3.0788e+03, -2.1263e+03, -2.6534e+03, -1.3812e+03, -2.6974e+03,\n",
      "         -3.2476e+03, -1.7746e+02, -3.1992e+03, -3.8886e+03],\n",
      "        [-4.6645e+05, -3.1474e+05, -3.9624e+05, -2.1028e+05, -4.0640e+05,\n",
      "         -4.8562e+05, -2.3243e+04, -4.8456e+05, -5.8395e+05],\n",
      "        [-4.1311e+06, -2.7876e+06, -3.5094e+06, -1.8624e+06, -3.5992e+06,\n",
      "         -4.3009e+06, -2.0570e+05, -4.2914e+06, -5.1718e+06],\n",
      "        [-2.7501e+04, -1.8570e+04, -2.3379e+04, -1.2398e+04, -2.3969e+04,\n",
      "         -2.8644e+04, -1.3790e+03, -2.8571e+04, -3.4442e+04],\n",
      "        [-3.5638e+06, -2.4048e+06, -3.0277e+06, -1.6066e+06, -3.1050e+06,\n",
      "         -3.7104e+06, -1.7740e+05, -3.7025e+06, -4.4620e+06]])\n",
      "Batch 1 ground truth:\n",
      " tensor([[0.6639, 0.6612, 0.6872, 0.0393, 0.4206, 0.4590, 0.0202, 0.6676, 0.6465],\n",
      "        [0.6631, 0.6672, 0.6815, 0.0833, 0.4648, 0.5058, 0.0753, 0.6700, 0.6271],\n",
      "        [0.6557, 0.6479, 0.6575, 0.1135, 0.3275, 0.3611, 0.0961, 0.6763, 0.6428],\n",
      "        [0.6227, 0.6279, 0.6299, 0.0952, 0.3386, 0.3752, 0.1304, 0.6748, 0.5377],\n",
      "        [0.5975, 0.6279, 0.6220, 0.0916, 0.6100, 0.6504, 0.1671, 0.6651, 0.4342],\n",
      "        [0.6590, 0.6675, 0.6837, 0.1395, 0.4594, 0.5000, 0.0667, 0.6590, 0.5735],\n",
      "        [0.6745, 0.7223, 0.7015, 0.2778, 0.6151, 0.6665, 0.2430, 0.6597, 0.6043],\n",
      "        [0.7394, 0.7571, 0.7655, 0.2185, 0.5307, 0.5813, 0.1089, 0.6688, 0.7236],\n",
      "        [0.7553, 0.7623, 0.7727, 0.1898, 0.4214, 0.4590, 0.0998, 0.6957, 0.8849],\n",
      "        [0.7562, 0.7630, 0.7624, 0.0965, 0.3415, 0.3688, 0.1512, 0.7273, 0.9507],\n",
      "        [0.7291, 0.7364, 0.7527, 0.0610, 0.4627, 0.5051, 0.0704, 0.7492, 0.7375],\n",
      "        [0.7392, 0.7347, 0.7518, 0.0652, 0.4155, 0.4526, 0.0667, 0.7642, 0.7402],\n",
      "        [0.7456, 0.7469, 0.7722, 0.0512, 0.4481, 0.4891, 0.0294, 0.7690, 0.5817],\n",
      "        [0.7464, 0.7450, 0.7401, 0.1120, 0.3352, 0.3624, 0.1701, 0.7664, 0.5162],\n",
      "        [0.7113, 0.7269, 0.7401, 0.0478, 0.5052, 0.5512, 0.0845, 0.7605, 0.4478],\n",
      "        [0.7285, 0.7242, 0.7505, 0.0328, 0.4379, 0.4776, 0.0233, 0.7586, 0.5777]])\n",
      "Batch 2 predictions:\n",
      " tensor([[-1.7145e+05, -1.1588e+05, -1.4570e+05, -7.7277e+04, -1.4936e+05,\n",
      "         -1.7866e+05, -8.4046e+03, -1.7808e+05, -2.1473e+05],\n",
      "        [-1.8802e+03, -1.2757e+03, -1.6028e+03, -8.4862e+02, -1.6423e+03,\n",
      "         -1.9603e+03, -9.7059e+01, -1.9547e+03, -2.3580e+03],\n",
      "        [-2.7478e+05, -1.8545e+05, -2.3341e+05, -1.2387e+05, -2.3939e+05,\n",
      "         -2.8610e+05, -1.3663e+04, -2.8538e+05, -3.4398e+05],\n",
      "        [-1.9437e+05, -1.3114e+05, -1.6508e+05, -8.7618e+04, -1.6934e+05,\n",
      "         -2.0235e+05, -9.6672e+03, -2.0188e+05, -2.4329e+05],\n",
      "        [-2.1298e+07, -1.4373e+07, -1.8094e+07, -9.6016e+06, -1.8556e+07,\n",
      "         -2.2175e+07, -1.0602e+06, -2.2125e+07, -2.6665e+07],\n",
      "        [-4.5093e+05, -3.0433e+05, -3.8306e+05, -2.0333e+05, -3.9280e+05,\n",
      "         -4.6943e+05, -2.2335e+04, -4.6834e+05, -5.6450e+05],\n",
      "        [-8.3118e+03, -5.6133e+03, -7.0656e+03, -3.7492e+03, -7.2461e+03,\n",
      "         -8.6528e+03, -4.1805e+02, -8.6330e+03, -1.0407e+04],\n",
      "        [-2.9499e+03, -1.9938e+03, -2.5073e+03, -1.3310e+03, -2.5740e+03,\n",
      "         -3.0705e+03, -1.5089e+02, -3.0636e+03, -3.6915e+03],\n",
      "        [-4.6789e+07, -3.1573e+07, -3.9748e+07, -2.1093e+07, -4.0764e+07,\n",
      "         -4.8714e+07, -2.3290e+06, -4.8604e+07, -5.8577e+07],\n",
      "        [-5.5088e+03, -3.7245e+03, -4.6836e+03, -2.4859e+03, -4.8022e+03,\n",
      "         -5.7350e+03, -2.7371e+02, -5.7200e+03, -6.8973e+03],\n",
      "        [-1.9784e+03, -1.3406e+03, -1.6839e+03, -8.9417e+02, -1.7265e+03,\n",
      "         -2.0592e+03, -1.0008e+02, -2.0542e+03, -2.4770e+03],\n",
      "        [-2.2030e+03, -1.4951e+03, -1.8771e+03, -9.9514e+02, -1.9232e+03,\n",
      "         -2.2958e+03, -1.1131e+02, -2.2873e+03, -2.7608e+03],\n",
      "        [-2.7404e+02, -1.9602e+02, -2.5697e+02, -1.3444e+02, -2.2667e+02,\n",
      "         -2.7526e+02,  1.3179e+01, -2.8081e+02, -3.4668e+02],\n",
      "        [-6.8935e+03, -4.7290e+03, -5.8992e+03, -3.1099e+03, -5.9802e+03,\n",
      "         -7.2121e+03, -3.1737e+02, -7.1852e+03, -8.7139e+03],\n",
      "        [-5.0946e+03, -3.4557e+03, -4.3457e+03, -2.3040e+03, -4.4374e+03,\n",
      "         -5.3120e+03, -2.4563e+02, -5.2938e+03, -6.3930e+03],\n",
      "        [-4.3146e+04, -2.9123e+04, -3.6636e+04, -1.9433e+04, -3.7607e+04,\n",
      "         -4.4945e+04, -2.1621e+03, -4.4802e+04, -5.3996e+04]])\n",
      "Batch 2 ground truth:\n",
      " tensor([[0.7372, 0.7431, 0.7647, 0.0471, 0.4476, 0.4885, 0.0459, 0.7574, 0.5580],\n",
      "        [0.7305, 0.7322, 0.7434, 0.0649, 0.3762, 0.4091, 0.0942, 0.7584, 0.5835],\n",
      "        [0.7216, 0.7267, 0.7434, 0.0663, 0.4669, 0.5096, 0.0679, 0.7519, 0.5264],\n",
      "        [0.7248, 0.7225, 0.7497, 0.0246, 0.4217, 0.4597, 0.0190, 0.7528, 0.6310],\n",
      "        [0.7292, 0.7388, 0.7543, 0.0474, 0.4292, 0.4680, 0.0747, 0.7518, 0.5647],\n",
      "        [0.7335, 0.7304, 0.7551, 0.0239, 0.4126, 0.4494, 0.0312, 0.7516, 0.5578],\n",
      "        [0.7373, 0.7392, 0.7608, 0.0447, 0.4436, 0.4840, 0.0459, 0.7492, 0.5300],\n",
      "        [0.7451, 0.7420, 0.7658, 0.0321, 0.4184, 0.4558, 0.0361, 0.7538, 0.5835],\n",
      "        [0.7540, 0.7703, 0.7816, 0.0658, 0.4911, 0.5378, 0.0961, 0.7565, 0.5450],\n",
      "        [0.7879, 0.7954, 0.8010, 0.1448, 0.3551, 0.3822, 0.1242, 0.7659, 0.6001],\n",
      "        [0.7835, 0.7862, 0.8082, 0.0697, 0.4322, 0.4712, 0.0471, 0.7748, 0.5413],\n",
      "        [0.7764, 0.7792, 0.7947, 0.0629, 0.4433, 0.4840, 0.0771, 0.7863, 0.5568],\n",
      "        [0.7902, 0.7886, 0.8132, 0.0642, 0.4189, 0.4558, 0.0355, 0.7946, 0.5566],\n",
      "        [0.7833, 0.7844, 0.8086, 0.0300, 0.4449, 0.4859, 0.0367, 0.8041, 0.5569],\n",
      "        [0.7940, 0.7892, 0.8174, 0.0327, 0.3935, 0.4264, 0.0184, 0.8082, 0.5203],\n",
      "        [0.7823, 0.7834, 0.7696, 0.0198, 0.4255, 0.4635, 0.2142, 0.8113, 0.5508]])\n",
      "Batch 3 predictions:\n",
      " tensor([[-4.6018e+06, -3.1054e+06, -3.9095e+06, -2.0745e+06, -4.0093e+06,\n",
      "         -4.7912e+06, -2.2910e+05, -4.7805e+06, -5.7614e+06],\n",
      "        [-8.0247e+06, -5.4156e+06, -6.8177e+06, -3.6177e+06, -6.9914e+06,\n",
      "         -8.3554e+06, -3.9928e+05, -8.3362e+06, -1.0047e+07],\n",
      "        [-1.8315e+03, -1.2437e+03, -1.5609e+03, -8.2805e+02, -1.5985e+03,\n",
      "         -1.9081e+03, -9.2660e+01, -1.9019e+03, -2.2956e+03],\n",
      "        [-1.7892e+03, -1.2152e+03, -1.5256e+03, -8.0864e+02, -1.5622e+03,\n",
      "         -1.8646e+03, -9.1236e+01, -1.8582e+03, -2.2434e+03],\n",
      "        [-2.2735e+03, -1.5427e+03, -1.9393e+03, -1.0253e+03, -1.9855e+03,\n",
      "         -2.3720e+03, -1.1675e+02, -2.3636e+03, -2.8532e+03],\n",
      "        [-5.9736e+03, -4.0404e+03, -5.0807e+03, -2.6956e+03, -5.2081e+03,\n",
      "         -6.2215e+03, -2.9953e+02, -6.2033e+03, -7.4813e+03],\n",
      "        [-3.7564e+03, -2.5460e+03, -3.2011e+03, -1.6956e+03, -3.2763e+03,\n",
      "         -3.9161e+03, -1.8716e+02, -3.9035e+03, -4.7113e+03],\n",
      "        [-2.9587e+04, -1.9980e+04, -2.5151e+04, -1.3341e+04, -2.5786e+04,\n",
      "         -3.0815e+04, -1.4824e+03, -3.0740e+04, -3.7055e+04],\n",
      "        [-5.7568e+03, -3.8902e+03, -4.8930e+03, -2.5970e+03, -5.0195e+03,\n",
      "         -5.9932e+03, -2.9155e+02, -5.9787e+03, -7.2074e+03],\n",
      "        [-4.5073e+02, -3.0971e+02, -3.8637e+02, -2.0320e+02, -3.9315e+02,\n",
      "         -4.6884e+02, -2.3425e+01, -4.6339e+02, -5.5899e+02],\n",
      "        [-5.0391e+03, -3.4041e+03, -4.2829e+03, -2.2732e+03, -4.3929e+03,\n",
      "         -5.2436e+03, -2.5107e+02, -5.2314e+03, -6.3070e+03],\n",
      "        [-7.7291e+04, -5.2126e+04, -6.5521e+04, -3.4823e+04, -6.7310e+04,\n",
      "         -8.0411e+04, -3.7489e+03, -8.0146e+04, -9.6586e+04],\n",
      "        [-1.8827e+03, -1.2798e+03, -1.6038e+03, -8.5077e+02, -1.6437e+03,\n",
      "         -1.9608e+03, -9.3366e+01, -1.9514e+03, -2.3565e+03],\n",
      "        [-6.8323e+04, -4.6118e+04, -5.8031e+04, -3.0793e+04, -5.9541e+04,\n",
      "         -7.1144e+04, -3.4151e+03, -7.0953e+04, -8.5520e+04],\n",
      "        [-2.5551e+03, -1.7318e+03, -2.1725e+03, -1.1542e+03, -2.2282e+03,\n",
      "         -2.6592e+03, -1.2610e+02, -2.6483e+03, -3.1967e+03],\n",
      "        [-9.0942e+03, -6.1414e+03, -7.7281e+03, -4.1036e+03, -7.9269e+03,\n",
      "         -9.4659e+03, -4.5659e+02, -9.4467e+03, -1.1385e+04]])\n",
      "Batch 3 ground truth:\n",
      " tensor([[0.7773, 0.7785, 0.8027, 0.0315, 0.4439, 0.4846, 0.0361, 0.8107, 0.5455],\n",
      "        [0.7767, 0.7724, 0.8021, 0.0288, 0.4210, 0.4584, 0.0104, 0.8109, 0.5459],\n",
      "        [0.7804, 0.7831, 0.8115, 0.0298, 0.4633, 0.5070, 0.0171, 0.8082, 0.5476],\n",
      "        [0.7911, 0.7911, 0.8155, 0.0996, 0.4493, 0.4910, 0.0361, 0.8084, 0.5622],\n",
      "        [0.7451, 0.7568, 0.7632, 0.1507, 0.5058, 0.5538, 0.1181, 0.8105, 0.6065],\n",
      "        [0.7587, 0.7588, 0.7735, 0.0611, 0.3813, 0.4136, 0.0796, 0.8064, 0.6702],\n",
      "        [0.7395, 0.7504, 0.7650, 0.0705, 0.4241, 0.4622, 0.0796, 0.7995, 0.6205],\n",
      "        [0.7367, 0.7378, 0.7617, 0.0380, 0.4264, 0.4648, 0.0349, 0.7918, 0.6230],\n",
      "        [0.7343, 0.7318, 0.7268, 0.0874, 0.3194, 0.3457, 0.1701, 0.7808, 0.5936],\n",
      "        [0.7082, 0.7135, 0.7359, 0.0434, 0.4438, 0.4840, 0.0404, 0.7626, 0.4906],\n",
      "        [0.7029, 0.7020, 0.7140, 0.0394, 0.3651, 0.3982, 0.0887, 0.7519, 0.4414],\n",
      "        [0.6719, 0.6864, 0.6951, 0.0505, 0.4513, 0.4917, 0.1028, 0.7395, 0.4286],\n",
      "        [0.6915, 0.6898, 0.7128, 0.0554, 0.4523, 0.4930, 0.0361, 0.7264, 0.4502],\n",
      "        [0.6944, 0.7054, 0.7159, 0.0313, 0.4771, 0.5198, 0.0955, 0.7180, 0.4707],\n",
      "        [0.6890, 0.7002, 0.6943, 0.1310, 0.4921, 0.5359, 0.1720, 0.7168, 0.5959],\n",
      "        [0.7043, 0.6972, 0.6653, 0.1252, 0.2389, 0.2606, 0.2938, 0.7149, 0.6341]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "denoise_net.eval()\n",
    "\n",
    "# Lists to store losses, predictions, and ground truths\n",
    "all_losses = []\n",
    "all_predictions = []\n",
    "all_ground_truths = []\n",
    "\n",
    "# Loop over the test DataLoader\n",
    "with torch.no_grad():\n",
    "    for x0 in test_loader:\n",
    "        # Ensure x0 is of shape [batch_size, dim]\n",
    "        if x0.dim() == 9:\n",
    "            x0 = x0.unsqueeze(1)\n",
    "        batch_size = x0.shape[0]\n",
    "\n",
    "        # Initialize x_T as pure Gaussian noise (same shape as x0)\n",
    "        x_T = torch.randn(batch_size, dim)\n",
    "\n",
    "        # Run the full reverse diffusion chain from t = num_diffusion_steps - 1 down to 0\n",
    "        x_t = x_T\n",
    "        for t_val in reversed(range(num_diffusion_steps)):\n",
    "            # Create a timestep tensor for the current reverse step (shape: [batch_size])\n",
    "            t_tensor = torch.full((batch_size,), t_val, dtype=torch.long)\n",
    "            # Perform one reverse diffusion step\n",
    "            x_t = reverse_diffusion_sample(x_t, betas, t_tensor, embedding_dim, denoise_net)\n",
    "        \n",
    "        # The final x_t is the predicted clean sample x0_pred\n",
    "        x0_pred = x_t\n",
    "\n",
    "        # Compute the MSE loss between the predicted clean sample and the ground truth x0\n",
    "        loss = F.mse_loss(x0_pred, x0)\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        # Collect predictions and ground truth for further analysis (if needed)\n",
    "        all_predictions.append(x0_pred)\n",
    "        all_ground_truths.append(x0)\n",
    "\n",
    "# Compute the average test MSE over all batches\n",
    "average_loss = sum(all_losses) / len(all_losses)\n",
    "print(\"Average Test MSE:\", average_loss)\n",
    "\n",
    "# Inspect a few predictions vs. ground truth:\n",
    "for i in range(min(3, len(all_predictions))):\n",
    "    print(f\"Batch {i+1} predictions:\\n\", all_predictions[i])\n",
    "    print(f\"Batch {i+1} ground truth:\\n\", all_ground_truths[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
