{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_all_stock_data\n",
    "import pandas as pd\n",
    "from dataset import ConditionalStockDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from multi_stock_input_modules import ResidualMLPWithExtraBlock as ResidualMLP\n",
    "from multi_stock_input_modules import forward_diffusion_sample\n",
    "from multi_stock_input_modules import reverse_diffusion_sample\n",
    "import torch.nn.functional as F\n",
    "from multi_stock_input_modules import Context_Encoder\n",
    "from multi_stock_input_modules import cosine_beta_schedule\n",
    "from multi_stock_input_modules import get_time_embedding\n",
    "from multi_stock_input_modules import TickerEmbedding\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and processed data for 88 stocks\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../../../price/raw/\"\n",
    "data = load_all_stock_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = 10\n",
    "feature_columns = [\"Open\",\"High\",\"Low\",\"Volume\",\"Return\",\"Diff\",\"HL_Diff\",\"MA5\",\"Return_MA5\"]\n",
    "target_column = [\"Close\"]\n",
    "train_data = ConditionalStockDataset(data,context_len,feature_columns,target_column,split='train')\n",
    "test_data = ConditionalStockDataset(data,context_len,feature_columns,target_column,split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA tensor([[3.0266e-04, 3.0252e-04, 3.0409e-04, 6.4080e-03, 7.3007e-01, 5.8009e-01,\n",
      "         4.5562e-05, 3.0576e-04, 6.3183e-01],\n",
      "        [3.0371e-04, 3.0257e-04, 3.0256e-04, 7.1370e-03, 7.1718e-01, 5.8004e-01,\n",
      "         7.7105e-05, 3.0611e-04, 6.4068e-01],\n",
      "        [3.0042e-04, 3.0392e-04, 3.0097e-04, 8.6181e-03, 7.4949e-01, 5.8016e-01,\n",
      "         1.3581e-04, 3.0527e-04, 6.2808e-01],\n",
      "        [3.0630e-04, 3.0584e-04, 3.0344e-04, 1.0590e-02, 7.1100e-01, 5.8002e-01,\n",
      "         1.2530e-04, 3.0418e-04, 6.3388e-01],\n",
      "        [3.0082e-04, 2.9882e-04, 2.9856e-04, 1.1218e-02, 7.0810e-01, 5.8001e-01,\n",
      "         8.1486e-05, 3.0276e-04, 6.2449e-01],\n",
      "        [2.9683e-04, 2.9725e-04, 2.9711e-04, 8.9039e-03, 7.4232e-01, 5.8013e-01,\n",
      "         7.8858e-05, 3.0175e-04, 6.2894e-01],\n",
      "        [3.0069e-04, 2.9847e-04, 2.9851e-04, 8.4224e-03, 7.0833e-01, 5.8001e-01,\n",
      "         7.5353e-05, 3.0103e-04, 6.3546e-01],\n",
      "        [2.9459e-04, 2.9463e-04, 2.9478e-04, 7.9062e-03, 7.4025e-01, 5.8012e-01,\n",
      "         7.2724e-05, 3.0009e-04, 6.3075e-01],\n",
      "        [2.9784e-04, 2.9590e-04, 2.9671e-04, 2.1344e-02, 7.2533e-01, 5.8007e-01,\n",
      "         5.9581e-05, 2.9858e-04, 6.2583e-01],\n",
      "        [2.9503e-04, 2.9603e-04, 2.9557e-04, 8.1702e-03, 7.4216e-01, 5.8013e-01,\n",
      "         8.4991e-05, 2.9743e-04, 6.3345e-01]]) tensor([0.0003])\n"
     ]
    }
   ],
   "source": [
    "for ticker,context,target in train_data :\n",
    "    print(ticker,context,target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85767 21487\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 16, shuffle = True)\n",
    "test_laoder = DataLoader(test_data, batch_size =16, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of context: torch.Size([16, 10, 9]) \n",
      " Shape of x0: torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "for sample in train_loader :\n",
    "    print(f\" Shape of context: {sample[1].shape} \\n Shape of x0: {sample[2].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.019402045756578445, LR: 0.0001\n",
      "Epoch 2, Loss: 0.006481352727860212, LR: 0.0001\n",
      "Epoch 3, Loss: 0.004343767650425434, LR: 0.0001\n",
      "Epoch 4, Loss: 0.0061234901659190655, LR: 0.0001\n",
      "Epoch 5, Loss: 0.0016276149544864893, LR: 0.0001\n",
      "Epoch 6, Loss: 0.03131469339132309, LR: 0.0001\n",
      "Epoch 7, Loss: 0.012462134473025799, LR: 0.0001\n",
      "Epoch 8, Loss: 0.0020250563975423574, LR: 0.0001\n",
      "Epoch 9, Loss: 0.0037357774563133717, LR: 0.0001\n",
      "Epoch 10, Loss: 0.0014304263750091195, LR: 5e-05\n",
      "Epoch 11, Loss: 0.000854013953357935, LR: 5e-05\n",
      "Epoch 12, Loss: 0.009855100885033607, LR: 5e-05\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_diffusion_steps = 100  # Total diffusion steps\n",
    "num_epochs = 200\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create a beta schedule: linearly spaced between 0.0001 and 0.01\n",
    "# betas = torch.linspace(0.0001, 0.01, num_diffusion_steps)\n",
    "betas = cosine_beta_schedule(num_diffusion_steps, s=0.008)\n",
    "\n",
    "\n",
    "# Since our targets are scalars, dim = 1\n",
    "dim = 1\n",
    "embedding_dim = 32  # As used in get_time_embedding\n",
    "#hidden_size for context = context_embedding size\n",
    "context_embedding_size = 32\n",
    "ticker_embedding_dim = 32\n",
    "\n",
    "# Denoise net parameters : dim, embedding_dim, context_embedding_size, hidden_size=512, num_chunks=8, attn_heads=4, dropout_prob=0.1\n",
    "\n",
    "denoise_net = ResidualMLP(dim=1, embedding_dim=32, context_embedding_size=32, ticker_embedding_size=ticker_embedding_dim, hidden_size=512, num_chunks=8, attn_heads=4,dropout_prob=0.1)\n",
    "\n",
    "#input size = num of features\n",
    "\n",
    "input_size = 9\n",
    "context_input_dim = 9\n",
    "context_hidden_dim = 32\n",
    "\n",
    "\n",
    "# Context encoding parameters : input_dim, hidden_dim, kernel_size=3, dilation_rates=[1, 2, 4], num_heads=4\n",
    "context_net = Context_Encoder(input_dim=context_input_dim, hidden_dim=context_hidden_dim, kernel_size=3, dilation_rates=[1,2,4], num_heads=4)\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(denoise_net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# To generate the ticker embeddings, we need to get a list of all the inique tickers in the dataset\n",
    "\n",
    "ticker_list =[tickers for tickers,_,_ in train_data]\n",
    "unique_tickers = [ticker for ticker in set(ticker_list)]\n",
    "\n",
    "\n",
    "ticker_emb_layer = TickerEmbedding(ticker_list = unique_tickers, ticker_embedding_dim=32)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for ticker,context,x0 in train_loader:\n",
    "        # Again, ticker is name of stock\n",
    "        # Context is a tensor of [batch_size, context_len=10, feature_dim=9]\n",
    "        # x0 is a tensor of [batch_size,dim=1]\n",
    "\n",
    "        # Ensure x0 is shaped as [batch_size, dim]\n",
    "        # context is of shape [batch_size, context_len, num_features]\n",
    "        \n",
    "        # Sample a random diffusion timestep for each sample in the batch\n",
    "        batch_size = x0.shape[0]\n",
    "        t = torch.randint(0, num_diffusion_steps, (batch_size,), dtype=torch.long)\n",
    "        \n",
    "        # Generate the noisy sample and the true noise using forward diffusion\n",
    "        x_t, true_noise = forward_diffusion_sample(x0, t, betas)\n",
    "        # print(f\"x_t shape : {x_t.shape}\")\n",
    "        \n",
    "        # Compute time embedding for the sampled timesteps\n",
    "        time_embedding = get_time_embedding(t, embedding_dim)\n",
    "        # print(f\"time_embedding shape : {time_embedding.shape}\")\n",
    "        \n",
    "        # Concatenate x_t with the time embedding to form the input to the denoising network\n",
    "        x_combined = torch.cat([x_t, time_embedding], dim=-1)\n",
    "        # x_combined shape is [batch_size, dim + embedding_dim]\n",
    "        # print(f\"x_combined shape : {x_combined.shape}\")\n",
    "        \n",
    "        # Get context embedding\n",
    "        context_embedding = context_net(context)\n",
    "        # print(f\"context_embedding shape :{context_embedding.shape}\")\n",
    "\n",
    "        x_combined = torch.cat([x_combined, context_embedding], dim=-1)\n",
    "        # print(f\"x_combined shape : {x_combined.shape}\")\n",
    "\n",
    "        ticker_emb = ticker_emb_layer(ticker) # Should be shape [num_tickers (should be batch_size),embedding_dim]\n",
    "        # print(f\"ticker embedding shape : {ticker_emb.shape}\")\n",
    "\n",
    "        x_combined = torch.cat([x_combined, ticker_emb], dim = -1)\n",
    "        # print(f\"x_combined after ticker emb : {x_combined.shape}\")\n",
    "        \n",
    "        # Predict the noise using the denoising network directly\n",
    "        predicted_noise = denoise_net(x_combined)\n",
    "        \n",
    "        # Compute the loss between predicted noise and true noise\n",
    "        loss = F.mse_loss(predicted_noise, true_noise)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(denoise_net.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}, LR: {current_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
