{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import forward_diffusion_sample\n",
    "import torch\n",
    "from modules import get_time_embedding\n",
    "from modules import model_architecture\n",
    "from modules import reverse_diffusion_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward diffusion test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_diffusion_steps = 1000\n",
    "betas = torch.linspace(0.0001, 0.02, num_diffusion_steps)\n",
    "\n",
    "batch_size =  4\n",
    "dim = 10\n",
    "\n",
    "x_0 = torch.randn(batch_size, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 50\n",
    "timestep_tensor = torch.full((batch_size,), timestep, dtype=torch.long) # Tensor shaped (batch_size,) with timestep repeated batch_size times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t,noise = forward_diffusion_sample(x_0, timestep_tensor,betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean sample (x_0):\n",
      "tensor([[ 1.4750, -0.3938, -0.4552, -1.6739,  0.8467,  1.0316,  0.9094, -0.4057,\n",
      "         -0.9085, -0.4406],\n",
      "        [ 0.8871,  1.2570, -0.2101, -1.3765,  0.6372,  0.5948, -1.3427,  2.4970,\n",
      "          1.5104,  1.5017],\n",
      "        [ 0.2432,  1.4318, -0.5506,  0.4336,  0.2242, -0.9029,  0.6744,  1.0569,\n",
      "          1.0807,  1.3416],\n",
      "        [ 0.1433,  0.9565, -0.8070,  0.1681, -0.4470,  0.6217,  1.5089,  0.6798,\n",
      "          0.2231, -0.0413]])\n",
      "\n",
      "Noisy sample (x_t):\n",
      "tensor([[ 1.2194, -0.4151, -0.7382, -1.3455,  0.5508,  0.7636,  1.1034, -0.2733,\n",
      "         -1.0079, -0.5609],\n",
      "        [ 0.6060,  1.1153, -0.0248, -1.4095,  0.5671,  0.7894, -1.1267,  2.5681,\n",
      "          1.2953,  1.4257],\n",
      "        [ 0.2092,  1.3063, -0.6171,  0.4502,  0.4828, -0.9745,  0.6418,  1.2699,\n",
      "          0.7880,  1.5301],\n",
      "        [ 0.3794,  1.2219, -0.6230,  0.1375, -0.7460,  0.7359,  1.1333,  0.6272,\n",
      "          0.1124, -0.1248]])\n",
      "\n",
      "Noise added:\n",
      "tensor([[-1.3452, -0.1572, -1.6727,  1.7481, -1.6329, -1.4560,  1.1982,  0.7279,\n",
      "         -0.6524, -0.7324],\n",
      "        [-1.5443, -0.7077,  1.0506, -0.3105, -0.3487,  1.1748,  1.1290,  0.6281,\n",
      "         -1.1086, -0.3070],\n",
      "        [-0.1747, -0.5986, -0.4313,  0.1338,  1.5110, -0.4920, -0.1291,  1.3209,\n",
      "         -1.5943,  1.2043],\n",
      "        [ 1.3743,  1.6142,  0.9907, -0.1618, -1.7635,  0.7135, -2.0350, -0.2438,\n",
      "         -0.6194, -0.4852]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Clean sample (x_0):\")\n",
    "print(x_0)\n",
    "print(\"\\nNoisy sample (x_t):\")\n",
    "print(x_t)\n",
    "print(\"\\nNoise added:\")\n",
    "print(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time embedding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_time_embedding(timestep_tensor, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2624,  0.1566, -0.1032,  0.5084, -0.9589,  0.3239,  0.9999,  0.7765,\n",
      "          0.4794,  0.2775,  0.1575,  0.0888,  0.0500,  0.0281,  0.0158,  0.0089,\n",
      "          0.9650, -0.9877, -0.9947, -0.8611,  0.2837, -0.9461, -0.0103,  0.6301,\n",
      "          0.8776,  0.9607,  0.9875,  0.9960,  0.9988,  0.9996,  0.9999,  1.0000],\n",
      "        [-0.2624,  0.1566, -0.1032,  0.5084, -0.9589,  0.3239,  0.9999,  0.7765,\n",
      "          0.4794,  0.2775,  0.1575,  0.0888,  0.0500,  0.0281,  0.0158,  0.0089,\n",
      "          0.9650, -0.9877, -0.9947, -0.8611,  0.2837, -0.9461, -0.0103,  0.6301,\n",
      "          0.8776,  0.9607,  0.9875,  0.9960,  0.9988,  0.9996,  0.9999,  1.0000],\n",
      "        [-0.2624,  0.1566, -0.1032,  0.5084, -0.9589,  0.3239,  0.9999,  0.7765,\n",
      "          0.4794,  0.2775,  0.1575,  0.0888,  0.0500,  0.0281,  0.0158,  0.0089,\n",
      "          0.9650, -0.9877, -0.9947, -0.8611,  0.2837, -0.9461, -0.0103,  0.6301,\n",
      "          0.8776,  0.9607,  0.9875,  0.9960,  0.9988,  0.9996,  0.9999,  1.0000],\n",
      "        [-0.2624,  0.1566, -0.1032,  0.5084, -0.9589,  0.3239,  0.9999,  0.7765,\n",
      "          0.4794,  0.2775,  0.1575,  0.0888,  0.0500,  0.0281,  0.0158,  0.0089,\n",
      "          0.9650, -0.9877, -0.9947, -0.8611,  0.2837, -0.9461, -0.0103,  0.6301,\n",
      "          0.8776,  0.9607,  0.9875,  0.9960,  0.9988,  0.9996,  0.9999,  1.0000]])\n",
      "torch.Size([4, 32])\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_T shape: torch.Size([4, 10])\n",
      "time_embedding shape: torch.Size([4, 2])\n",
      "x shape: torch.Size([4, 12])\n"
     ]
    }
   ],
   "source": [
    "x_t_minus_1 = reverse_diffusion_sample(x_t, betas, timestep_tensor, model_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original sample (x_t): tensor([[ 0.9129,  0.9456,  1.9236,  0.6603,  0.0957, -0.8940,  1.2359,  1.8455,\n",
      "          2.4707, -0.1314],\n",
      "        [ 0.4904,  0.2617, -0.1398, -0.3541,  0.0190,  0.6403,  1.2285,  0.5831,\n",
      "          0.2228,  0.3880],\n",
      "        [-0.7446, -0.3469, -1.2333,  0.9391, -1.0112,  1.4118,  0.7305,  0.3503,\n",
      "         -0.8563,  0.0370],\n",
      "        [ 1.6883,  1.1925, -0.6067, -0.5078, -0.9327, -0.2396,  1.2374, -0.2426,\n",
      "         -0.5686, -0.3173]])\n",
      " Noise added: tensor([[-0.5921,  0.8497,  1.6738,  0.4271, -1.4904, -0.8752, -1.5927, -0.7021,\n",
      "          0.9303,  0.0672],\n",
      "        [-0.7069,  0.5148, -0.2175,  0.2886,  0.1850,  0.9603, -0.7536, -0.7710,\n",
      "          1.1935,  1.2176],\n",
      "        [ 1.0512, -1.2152,  1.1807,  0.4453, -0.5517,  1.9664,  0.4839, -0.1953,\n",
      "          0.0798, -0.0605],\n",
      "        [ 0.0398,  0.3673,  0.4900, -0.7888, -0.4366,  0.5434, -1.3997,  0.6173,\n",
      "         -1.0215, -1.7815]])\n",
      " Reversed sample (x_t-1): tensor([[ 0.9119,  0.9473,  1.9252,  0.6607,  0.0961, -0.8930,  1.2362,  1.8470,\n",
      "          2.4713, -0.1303],\n",
      "        [ 0.4905,  0.2621, -0.1396, -0.3538,  0.0193,  0.6413,  1.2285,  0.5840,\n",
      "          0.2227,  0.3890],\n",
      "        [-0.7446, -0.3468, -1.2338,  0.9402, -1.0112,  1.4118,  0.7300,  0.3505,\n",
      "         -0.8563,  0.0376],\n",
      "        [ 1.6890,  1.1934, -0.6066, -0.5074, -0.9330, -0.2394,  1.2371, -0.2425,\n",
      "         -0.5695, -0.3162]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\" Original sample (x_t): {x_t}\")\n",
    "print(f\" Noise added: {noise}\")\n",
    "print(f\" Reversed sample (x_t-1): {x_t_minus_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from modules import run_reverse_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ConditionalStockDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258\n"
     ]
    }
   ],
   "source": [
    "base_path = \"../../price/raw\"\n",
    "df = pd.read_csv(f\"{base_path}/AAPL.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Daily Return (percentage change from open to close)\n",
    "df['Return'] = (df['Close'] - df['Open']) / df['Open']\n",
    "\n",
    "# 2. Price Difference (Close - Open)\n",
    "df['Diff'] = df['Close'] - df['Open']\n",
    "\n",
    "# 3. High-Low Difference (as a measure of volatility)\n",
    "df['HL_Diff'] = df['High'] - df['Low']\n",
    "\n",
    "# 4. 5-day Moving Average of the Close (shifted to avoid leakage)\n",
    "df['MA5'] = df['Close'].rolling(window=5).mean().shift(1)\n",
    "\n",
    "# 5. 5-day Moving Average of the Return (shifted)\n",
    "df['Return_MA5'] = df['Return'].rolling(window=5).mean().shift(1)\n",
    "\n",
    "# Drop rows with NaN values that result from rolling and shifting\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Return</th>\n",
       "      <th>Diff</th>\n",
       "      <th>HL_Diff</th>\n",
       "      <th>MA5</th>\n",
       "      <th>Return_MA5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-09-11</td>\n",
       "      <td>95.015717</td>\n",
       "      <td>95.728569</td>\n",
       "      <td>93.785713</td>\n",
       "      <td>94.370003</td>\n",
       "      <td>85.265068</td>\n",
       "      <td>125995800</td>\n",
       "      <td>-0.006796</td>\n",
       "      <td>-0.645714</td>\n",
       "      <td>1.942856</td>\n",
       "      <td>96.132857</td>\n",
       "      <td>-0.002394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-09-12</td>\n",
       "      <td>95.264282</td>\n",
       "      <td>95.699997</td>\n",
       "      <td>93.714287</td>\n",
       "      <td>95.684288</td>\n",
       "      <td>86.452538</td>\n",
       "      <td>178058300</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>0.420006</td>\n",
       "      <td>1.985710</td>\n",
       "      <td>95.722000</td>\n",
       "      <td>-0.006519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-09-13</td>\n",
       "      <td>96.767143</td>\n",
       "      <td>97.928574</td>\n",
       "      <td>96.395714</td>\n",
       "      <td>97.568573</td>\n",
       "      <td>88.155037</td>\n",
       "      <td>149590000</td>\n",
       "      <td>0.008282</td>\n",
       "      <td>0.801430</td>\n",
       "      <td>1.532860</td>\n",
       "      <td>95.709428</td>\n",
       "      <td>-0.004057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-09-14</td>\n",
       "      <td>98.565712</td>\n",
       "      <td>99.568573</td>\n",
       "      <td>98.269997</td>\n",
       "      <td>98.754288</td>\n",
       "      <td>89.226341</td>\n",
       "      <td>150118500</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.188576</td>\n",
       "      <td>1.298576</td>\n",
       "      <td>95.901143</td>\n",
       "      <td>-0.003321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-09-17</td>\n",
       "      <td>99.907143</td>\n",
       "      <td>99.971428</td>\n",
       "      <td>99.230003</td>\n",
       "      <td>99.968575</td>\n",
       "      <td>90.323479</td>\n",
       "      <td>99507800</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.061432</td>\n",
       "      <td>0.741425</td>\n",
       "      <td>96.210858</td>\n",
       "      <td>-0.003644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close  \\\n",
       "0  2012-09-11  95.015717  95.728569  93.785713  94.370003  85.265068   \n",
       "1  2012-09-12  95.264282  95.699997  93.714287  95.684288  86.452538   \n",
       "2  2012-09-13  96.767143  97.928574  96.395714  97.568573  88.155037   \n",
       "3  2012-09-14  98.565712  99.568573  98.269997  98.754288  89.226341   \n",
       "4  2012-09-17  99.907143  99.971428  99.230003  99.968575  90.323479   \n",
       "\n",
       "      Volume    Return      Diff   HL_Diff        MA5  Return_MA5  \n",
       "0  125995800 -0.006796 -0.645714  1.942856  96.132857   -0.002394  \n",
       "1  178058300  0.004409  0.420006  1.985710  95.722000   -0.006519  \n",
       "2  149590000  0.008282  0.801430  1.532860  95.709428   -0.004057  \n",
       "3  150118500  0.001913  0.188576  1.298576  95.901143   -0.003321  \n",
       "4   99507800  0.000615  0.061432  0.741425  96.210858   -0.003644  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['Open', 'High', 'Low', 'Volume', 'Return', 'Diff', 'HL_Diff', 'MA5', 'Return_MA5']\n",
    "# We keep the target as the raw 'Close' (normalized later)\n",
    "target_column = 'Close'\n",
    "\n",
    "filtered_df = df[feature_columns + [target_column]]\n",
    "\n",
    "seq_len = 10\n",
    "\n",
    "# Split into training and test data \n",
    "train_size = int(len(filtered_df) * 0.8)\n",
    "train_data = filtered_df.iloc[:train_size].copy()\n",
    "test_data = filtered_df.iloc[train_size:].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Return</th>\n",
       "      <th>Diff</th>\n",
       "      <th>HL_Diff</th>\n",
       "      <th>MA5</th>\n",
       "      <th>Return_MA5</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95.015717</td>\n",
       "      <td>95.728569</td>\n",
       "      <td>93.785713</td>\n",
       "      <td>125995800</td>\n",
       "      <td>-0.006796</td>\n",
       "      <td>-0.645714</td>\n",
       "      <td>1.942856</td>\n",
       "      <td>96.132857</td>\n",
       "      <td>-0.002394</td>\n",
       "      <td>94.370003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.264282</td>\n",
       "      <td>95.699997</td>\n",
       "      <td>93.714287</td>\n",
       "      <td>178058300</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>0.420006</td>\n",
       "      <td>1.985710</td>\n",
       "      <td>95.722000</td>\n",
       "      <td>-0.006519</td>\n",
       "      <td>95.684288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96.767143</td>\n",
       "      <td>97.928574</td>\n",
       "      <td>96.395714</td>\n",
       "      <td>149590000</td>\n",
       "      <td>0.008282</td>\n",
       "      <td>0.801430</td>\n",
       "      <td>1.532860</td>\n",
       "      <td>95.709428</td>\n",
       "      <td>-0.004057</td>\n",
       "      <td>97.568573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98.565712</td>\n",
       "      <td>99.568573</td>\n",
       "      <td>98.269997</td>\n",
       "      <td>150118500</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.188576</td>\n",
       "      <td>1.298576</td>\n",
       "      <td>95.901143</td>\n",
       "      <td>-0.003321</td>\n",
       "      <td>98.754288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.907143</td>\n",
       "      <td>99.971428</td>\n",
       "      <td>99.230003</td>\n",
       "      <td>99507800</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.061432</td>\n",
       "      <td>0.741425</td>\n",
       "      <td>96.210858</td>\n",
       "      <td>-0.003644</td>\n",
       "      <td>99.968575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>106.620003</td>\n",
       "      <td>107.440002</td>\n",
       "      <td>106.290001</td>\n",
       "      <td>24970300</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.199997</td>\n",
       "      <td>1.150001</td>\n",
       "      <td>107.980000</td>\n",
       "      <td>-0.001699</td>\n",
       "      <td>106.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>105.800003</td>\n",
       "      <td>106.500000</td>\n",
       "      <td>105.500000</td>\n",
       "      <td>24863900</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.199997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>107.642000</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>105.660004</td>\n",
       "      <td>106.570000</td>\n",
       "      <td>105.639999</td>\n",
       "      <td>29662400</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.439994</td>\n",
       "      <td>0.930001</td>\n",
       "      <td>107.072000</td>\n",
       "      <td>-0.000781</td>\n",
       "      <td>106.099998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>106.139999</td>\n",
       "      <td>106.800003</td>\n",
       "      <td>105.620003</td>\n",
       "      <td>26701500</td>\n",
       "      <td>0.005559</td>\n",
       "      <td>0.590004</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>106.686000</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>106.730003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>107.699997</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>106.820000</td>\n",
       "      <td>26802500</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.030006</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>106.518001</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>107.730003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1002 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open        High         Low     Volume    Return      Diff  \\\n",
       "0      95.015717   95.728569   93.785713  125995800 -0.006796 -0.645714   \n",
       "1      95.264282   95.699997   93.714287  178058300  0.004409  0.420006   \n",
       "2      96.767143   97.928574   96.395714  149590000  0.008282  0.801430   \n",
       "3      98.565712   99.568573   98.269997  150118500  0.001913  0.188576   \n",
       "4      99.907143   99.971428   99.230003   99507800  0.000615  0.061432   \n",
       "...          ...         ...         ...        ...       ...       ...   \n",
       "997   106.620003  107.440002  106.290001   24970300  0.001876  0.199997   \n",
       "998   105.800003  106.500000  105.500000   24863900  0.001890  0.199997   \n",
       "999   105.660004  106.570000  105.639999   29662400  0.004164  0.439994   \n",
       "1000  106.139999  106.800003  105.620003   26701500  0.005559  0.590004   \n",
       "1001  107.699997  108.000000  106.820000   26802500  0.000279  0.030006   \n",
       "\n",
       "       HL_Diff         MA5  Return_MA5       Close  \n",
       "0     1.942856   96.132857   -0.002394   94.370003  \n",
       "1     1.985710   95.722000   -0.006519   95.684288  \n",
       "2     1.532860   95.709428   -0.004057   97.568573  \n",
       "3     1.298576   95.901143   -0.003321   98.754288  \n",
       "4     0.741425   96.210858   -0.003644   99.968575  \n",
       "...        ...         ...         ...         ...  \n",
       "997   1.150001  107.980000   -0.001699  106.820000  \n",
       "998   1.000000  107.642000   -0.000681  106.000000  \n",
       "999   0.930001  107.072000   -0.000781  106.099998  \n",
       "1000  1.180000  106.686000    0.001046  106.730003  \n",
       "1001  1.180000  106.518001    0.001823  107.730003  \n",
       "\n",
       "[1002 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9174/3772158868.py:5: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.32072709 0.46856191 0.38772434 ... 0.0471822  0.03877453 0.03906133]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train_data.loc[:, train_data.columns] = scaler.transform(train_data)\n",
      "/tmp/ipykernel_9174/3772158868.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.03928253  0.08325006  0.1134565   0.0951555   0.09156573  0.13950723\n",
      "  0.27782951  0.21846813  0.1897979   0.09647874  0.06095948  0.06518731\n",
      "  0.05119053  0.11197765  0.04776999  0.0328282   0.04712172  0.06485735\n",
      "  0.0662547   0.02457757  0.04739346  0.02387137  0.04467458  0.03212115\n",
      "  0.06584836  0.14480245  0.06968404  0.06288499  0.06419062  0.03003833\n",
      "  0.03267515  0.01984344  0.03146067  0.02881107  0.02979356  0.0996193\n",
      "  0.15074622  0.06109493  0.07046464  0.03797349  0.08740009  0.04340359\n",
      "  0.03943076  0.05051755  0.05541012  0.03125821  0.1309891   0.125191\n",
      "  0.0597663   0.10827003  0.05457103  0.13003529  0.04141675  0.0436796\n",
      "  0.04605262  0.03668462  0.04083294 -0.00445954  0.04017302  0.04396327\n",
      "  0.06563909  0.06826455  0.03828187  0.06042053  0.03733772  0.04813715\n",
      "  0.03981609  0.0606423   0.03784571  0.08713885  0.05958939  0.09506322\n",
      "  0.08889171  0.0418353   0.02379157  0.03048783  0.0370265   0.00322234\n",
      "  0.01490913  0.02231756  0.00565954  0.04980567  0.04468196  0.02292011\n",
      "  0.02597406  0.05311547  0.05825508  0.03241561  0.04129351  0.03986692\n",
      "  0.03710033  0.06074794  0.0302885   0.03563909  0.05551774  0.02556687\n",
      "  0.02886304  0.05489219  0.03774122  0.02134359  0.04921277  0.10266331\n",
      "  0.28094252  0.05867676  0.03254396  0.03918457  0.07137926  0.02827553\n",
      "  0.04345527  0.01993119  0.02836441  0.05730184  0.06410799  0.02708433\n",
      "  0.02598712  0.03254368  0.02212163  0.02198334  0.02478996  0.0204761\n",
      "  0.02963511  0.06635551  0.03738173  0.02289172  0.02471443  0.0124938\n",
      "  0.01607421  0.02586701  0.01864572  0.01242395  0.00642508  0.03590743\n",
      "  0.01756441  0.08756819  0.0241238   0.0752016   0.03638561  0.02072854\n",
      "  0.02654765  0.02989692  0.0577238   0.04584079  0.0231734   0.01878457\n",
      "  0.01970459  0.01943682  0.04166067  0.02300785  0.01029569  0.01671652\n",
      "  0.04921817  0.02073904  0.01356318  0.01003985  0.0046884   0.01215901\n",
      "  0.02917141  0.01213772  0.01160786  0.01654075  0.01986218  0.00340719\n",
      "  0.02218836  0.0583715   0.0917344   0.09271348  0.02931992  0.04055267\n",
      "  0.10138948  0.0740672   0.03623085  0.04034652  0.05531642  0.03681012\n",
      "  0.01988291  0.10711206  0.05827297  0.03951083  0.02816848  0.01951491\n",
      "  0.01741107  0.01757463  0.02457558  0.02010554  0.03238466  0.00953441\n",
      "  0.0418106   0.0348849   0.03855702  0.02278239  0.02329692  0.14719251\n",
      "  0.16827514  0.05996876  0.05248878  0.05428963  0.10595749  0.05535731\n",
      "  0.03365934  0.02333952  0.01720748  0.06358636  0.03590913  0.03326692\n",
      "  0.0256583   0.05239848  0.02833232  0.00344127  0.02420217  0.03146919\n",
      "  0.01747837  0.02284202  0.0191256   0.03361505  0.03450923  0.0201203\n",
      "  0.03051708  0.01369352  0.02236611  0.0119185   0.03749985  0.02398523\n",
      "  0.01649077  0.00776507  0.05517245  0.01183332  0.01930762  0.06338532\n",
      "  0.16154395  0.03989843  0.02133507  0.02505603  0.06576289  0.03715598\n",
      "  0.07882034  0.03751263  0.0255044   0.04662309  0.0415292   0.04229304\n",
      "  0.04083776  0.03782896  0.02430156  0.0180389   0.01923095  0.03530629\n",
      "  0.03668604  0.04676904  0.04038769  0.03901192  0.00995665]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  test_data.loc[:, test_data.columns] = scaler.transform(test_data)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "\n",
    "# Transform train and test sets.\n",
    "train_data.loc[:, train_data.columns] = scaler.transform(train_data)\n",
    "test_data.loc[:, test_data.columns] = scaler.transform(test_data)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ConditionalStockDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "context_len = 10\n",
    "train_dataset = ConditionalStockDataset(train_data,context_len, feature_columns=feature_columns ,target_column=target_column)\n",
    "test_dataset = ConditionalStockDataset(test_data, context_len, feature_columns=feature_columns ,target_column=target_column)\n",
    "print(\"Datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.5041, 0.4985, 0.5066, 0.4686, 0.4615, 0.4987, 0.0934, 0.5208, 0.4064],\n",
      "        [0.5231, 0.5273, 0.5417, 0.3877, 0.4867, 0.5231, 0.0657, 0.5207, 0.4701],\n",
      "        [0.5458, 0.5485, 0.5663, 0.3892, 0.4452, 0.4839, 0.0513, 0.5232, 0.4892],\n",
      "        [0.5628, 0.5537, 0.5788, 0.2455, 0.4367, 0.4758, 0.0172, 0.5274, 0.4808],\n",
      "        [0.5638, 0.5584, 0.5822, 0.2281, 0.4516, 0.4904, 0.0235, 0.5416, 0.6188],\n",
      "        [0.5645, 0.5614, 0.5881, 0.1950, 0.4499, 0.4887, 0.0105, 0.5574, 0.6690],\n",
      "        [0.5625, 0.5542, 0.5770, 0.2019, 0.4284, 0.4676, 0.0282, 0.5698, 0.6598],\n",
      "        [0.5684, 0.5634, 0.5877, 0.3687, 0.4112, 0.4506, 0.0218, 0.5758, 0.6135],\n",
      "        [0.5402, 0.5451, 0.5571, 0.4171, 0.4700, 0.5078, 0.0778, 0.5792, 0.5865],\n",
      "        [0.5428, 0.5407, 0.5384, 0.3312, 0.2932, 0.3372, 0.1448, 0.5757, 0.6129]]), tensor(0.5082))\n"
     ]
    }
   ],
   "source": [
    "#Visualize the first sample\n",
    "sample = train_dataset[1]\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6639, 0.6612, 0.6872, 0.0393, 0.4206, 0.4590, 0.0202, 0.6676, 0.6465],\n",
       "         [0.6631, 0.6672, 0.6815, 0.0833, 0.4648, 0.5058, 0.0753, 0.6700, 0.6271],\n",
       "         [0.6557, 0.6479, 0.6575, 0.1135, 0.3275, 0.3611, 0.0961, 0.6763, 0.6428],\n",
       "         [0.6227, 0.6279, 0.6299, 0.0952, 0.3386, 0.3752, 0.1304, 0.6748, 0.5377],\n",
       "         [0.5975, 0.6279, 0.6220, 0.0916, 0.6100, 0.6504, 0.1671, 0.6651, 0.4342],\n",
       "         [0.6590, 0.6675, 0.6837, 0.1395, 0.4594, 0.5000, 0.0667, 0.6590, 0.5735],\n",
       "         [0.6745, 0.7223, 0.7015, 0.2778, 0.6151, 0.6665, 0.2430, 0.6597, 0.6043],\n",
       "         [0.7394, 0.7571, 0.7655, 0.2185, 0.5307, 0.5813, 0.1089, 0.6688, 0.7236],\n",
       "         [0.7553, 0.7623, 0.7727, 0.1898, 0.4214, 0.4590, 0.0998, 0.6957, 0.8849],\n",
       "         [0.7562, 0.7630, 0.7624, 0.0965, 0.3415, 0.3688, 0.1512, 0.7273, 0.9507]]),\n",
       " tensor(0.7483))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context shape : torch.Size([16, 10, 9]),\n",
      " x0 shape : torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for context,x0 in train_loader :\n",
    "    # print(x0)\n",
    "    print(f\"Context shape : {context.shape},\\n x0 shape : {x0.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import model_architecture\n",
    "from modules import forward_diffusion_sample\n",
    "from modules import reverse_diffusion_sample\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from modules import ResidualMLP\n",
    "from modules import Context_Encoder\n",
    "from modules import cosine_beta_schedule\n",
    "from modules import get_time_embedding\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.20619073510169983, LR: 0.001\n",
      "Epoch 2, Loss: 0.07394007593393326, LR: 0.001\n",
      "Epoch 3, Loss: 0.23605255782604218, LR: 0.001\n",
      "Epoch 4, Loss: 0.145208477973938, LR: 0.001\n",
      "Epoch 5, Loss: 0.2131229192018509, LR: 0.001\n",
      "Epoch 6, Loss: 0.08636755496263504, LR: 0.001\n",
      "Epoch 7, Loss: 0.02887505106627941, LR: 0.001\n",
      "Epoch 8, Loss: 0.09146641939878464, LR: 0.001\n",
      "Epoch 9, Loss: 0.11244304478168488, LR: 0.001\n",
      "Epoch 10, Loss: 0.06897285580635071, LR: 0.0005\n",
      "Epoch 11, Loss: 0.02845092862844467, LR: 0.0005\n",
      "Epoch 12, Loss: 0.029301658272743225, LR: 0.0005\n",
      "Epoch 13, Loss: 0.01658010482788086, LR: 0.0005\n",
      "Epoch 14, Loss: 0.05508243665099144, LR: 0.0005\n",
      "Epoch 15, Loss: 0.013810120522975922, LR: 0.0005\n",
      "Epoch 16, Loss: 0.025220485404133797, LR: 0.0005\n",
      "Epoch 17, Loss: 0.006653897929936647, LR: 0.0005\n",
      "Epoch 18, Loss: 0.003623660420998931, LR: 0.0005\n",
      "Epoch 19, Loss: 0.05558190122246742, LR: 0.0005\n",
      "Epoch 20, Loss: 0.09848027676343918, LR: 0.00025\n",
      "Epoch 21, Loss: 0.004261439666152, LR: 0.00025\n",
      "Epoch 22, Loss: 0.004988234955817461, LR: 0.00025\n",
      "Epoch 23, Loss: 0.15172317624092102, LR: 0.00025\n",
      "Epoch 24, Loss: 0.14417403936386108, LR: 0.00025\n",
      "Epoch 25, Loss: 0.0081975432112813, LR: 0.00025\n",
      "Epoch 26, Loss: 0.015449360013008118, LR: 0.00025\n",
      "Epoch 27, Loss: 0.004126536659896374, LR: 0.00025\n",
      "Epoch 28, Loss: 0.027822094038128853, LR: 0.00025\n",
      "Epoch 29, Loss: 0.07197026908397675, LR: 0.00025\n",
      "Epoch 30, Loss: 0.023961421102285385, LR: 0.000125\n",
      "Epoch 31, Loss: 0.0009643269004300237, LR: 0.000125\n",
      "Epoch 32, Loss: 0.005858364515006542, LR: 0.000125\n",
      "Epoch 33, Loss: 0.016374239698052406, LR: 0.000125\n",
      "Epoch 34, Loss: 0.002463590819388628, LR: 0.000125\n",
      "Epoch 35, Loss: 0.01384582556784153, LR: 0.000125\n",
      "Epoch 36, Loss: 0.0012704837135970592, LR: 0.000125\n",
      "Epoch 37, Loss: 0.0058294860646128654, LR: 0.000125\n",
      "Epoch 38, Loss: 0.005259586963802576, LR: 0.000125\n",
      "Epoch 39, Loss: 0.013602996245026588, LR: 0.000125\n",
      "Epoch 40, Loss: 0.035502731800079346, LR: 6.25e-05\n",
      "Epoch 41, Loss: 0.055708762258291245, LR: 6.25e-05\n",
      "Epoch 42, Loss: 0.0026643138844519854, LR: 6.25e-05\n",
      "Epoch 43, Loss: 0.0030811592005193233, LR: 6.25e-05\n",
      "Epoch 44, Loss: 0.005455235950648785, LR: 6.25e-05\n",
      "Epoch 45, Loss: 0.005053951404988766, LR: 6.25e-05\n",
      "Epoch 46, Loss: 0.01661897823214531, LR: 6.25e-05\n",
      "Epoch 47, Loss: 0.0214812234044075, LR: 6.25e-05\n",
      "Epoch 48, Loss: 0.012303890660405159, LR: 6.25e-05\n",
      "Epoch 49, Loss: 0.01542884111404419, LR: 6.25e-05\n",
      "Epoch 50, Loss: 0.004876571241766214, LR: 3.125e-05\n",
      "Epoch 51, Loss: 0.01071861945092678, LR: 3.125e-05\n",
      "Epoch 52, Loss: 0.0013201069086790085, LR: 3.125e-05\n",
      "Epoch 53, Loss: 0.020157281309366226, LR: 3.125e-05\n",
      "Epoch 54, Loss: 0.0004316342528909445, LR: 3.125e-05\n",
      "Epoch 55, Loss: 0.04594636708498001, LR: 3.125e-05\n",
      "Epoch 56, Loss: 0.07625152915716171, LR: 3.125e-05\n",
      "Epoch 57, Loss: 0.008147344924509525, LR: 3.125e-05\n",
      "Epoch 58, Loss: 0.012555756606161594, LR: 3.125e-05\n",
      "Epoch 59, Loss: 0.028687195852398872, LR: 3.125e-05\n",
      "Epoch 60, Loss: 0.000791427562944591, LR: 1.5625e-05\n",
      "Epoch 61, Loss: 0.002356312470510602, LR: 1.5625e-05\n",
      "Epoch 62, Loss: 0.001592558459378779, LR: 1.5625e-05\n",
      "Epoch 63, Loss: 0.018969308584928513, LR: 1.5625e-05\n",
      "Epoch 64, Loss: 0.0015917787095531821, LR: 1.5625e-05\n",
      "Epoch 65, Loss: 0.005742624867707491, LR: 1.5625e-05\n",
      "Epoch 66, Loss: 0.0018907587509602308, LR: 1.5625e-05\n",
      "Epoch 67, Loss: 0.041029300540685654, LR: 1.5625e-05\n",
      "Epoch 68, Loss: 0.002851134864613414, LR: 1.5625e-05\n",
      "Epoch 69, Loss: 0.0006569263059645891, LR: 1.5625e-05\n",
      "Epoch 70, Loss: 0.0082612419500947, LR: 7.8125e-06\n",
      "Epoch 71, Loss: 0.006424361374229193, LR: 7.8125e-06\n",
      "Epoch 72, Loss: 0.01775522530078888, LR: 7.8125e-06\n",
      "Epoch 73, Loss: 0.003437908599153161, LR: 7.8125e-06\n",
      "Epoch 74, Loss: 0.0004662135324906558, LR: 7.8125e-06\n",
      "Epoch 75, Loss: 0.0029834636952728033, LR: 7.8125e-06\n",
      "Epoch 76, Loss: 0.02185044251382351, LR: 7.8125e-06\n",
      "Epoch 77, Loss: 0.0008797856862656772, LR: 7.8125e-06\n",
      "Epoch 78, Loss: 0.1262257695198059, LR: 7.8125e-06\n",
      "Epoch 79, Loss: 0.00218419567681849, LR: 7.8125e-06\n",
      "Epoch 80, Loss: 0.03817696124315262, LR: 3.90625e-06\n",
      "Epoch 81, Loss: 0.022847997024655342, LR: 3.90625e-06\n",
      "Epoch 82, Loss: 0.0024986169300973415, LR: 3.90625e-06\n",
      "Epoch 83, Loss: 0.0016041347989812493, LR: 3.90625e-06\n",
      "Epoch 84, Loss: 0.029929572716355324, LR: 3.90625e-06\n",
      "Epoch 85, Loss: 0.0015826546587049961, LR: 3.90625e-06\n",
      "Epoch 86, Loss: 0.0005300032789818943, LR: 3.90625e-06\n",
      "Epoch 87, Loss: 0.0029693858232349157, LR: 3.90625e-06\n",
      "Epoch 88, Loss: 0.006932985968887806, LR: 3.90625e-06\n",
      "Epoch 89, Loss: 0.014042938128113747, LR: 3.90625e-06\n",
      "Epoch 90, Loss: 0.0200089979916811, LR: 1.953125e-06\n",
      "Epoch 91, Loss: 0.007318378426134586, LR: 1.953125e-06\n",
      "Epoch 92, Loss: 0.05781756713986397, LR: 1.953125e-06\n",
      "Epoch 93, Loss: 0.005122106987982988, LR: 1.953125e-06\n",
      "Epoch 94, Loss: 0.015906618908047676, LR: 1.953125e-06\n",
      "Epoch 95, Loss: 0.005145972594618797, LR: 1.953125e-06\n",
      "Epoch 96, Loss: 0.006859390065073967, LR: 1.953125e-06\n",
      "Epoch 97, Loss: 0.002443868201225996, LR: 1.953125e-06\n",
      "Epoch 98, Loss: 0.01630839705467224, LR: 1.953125e-06\n",
      "Epoch 99, Loss: 0.003002125769853592, LR: 1.953125e-06\n",
      "Epoch 100, Loss: 0.003481912426650524, LR: 9.765625e-07\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_diffusion_steps = 100  # Total diffusion steps\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Create a beta schedule: linearly spaced between 0.0001 and 0.01\n",
    "# betas = torch.linspace(0.0001, 0.01, num_diffusion_steps)\n",
    "betas = cosine_beta_schedule(num_diffusion_steps, s=0.008)\n",
    "\n",
    "\n",
    "# Since our targets are scalars, dim = 1\n",
    "dim = 1\n",
    "embedding_dim = 32  # As used in get_time_embedding\n",
    "#hidden_size for context = context_embedding size\n",
    "context_embedding_size = 32\n",
    "denoise_net = ResidualMLP(dim, embedding_dim, context_embedding_size=context_embedding_size, hidden_size=512)\n",
    "# Instantiate the context embedding network\n",
    "#input size = num of features\n",
    "input_size = 9\n",
    "# Instantiate the denoising network\n",
    "context_net = Context_Encoder(input_size, context_len=10,  context_embedding_size=context_embedding_size)\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(denoise_net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for context,x0 in train_loader:\n",
    "        # Ensure x0 is shaped as [batch_size, dim]\n",
    "        # If needed, reshape x0 (for instance, x0 = x0.unsqueeze(1) if x0 is 1D)\n",
    "        # context is of shape [batch_size, context_len, num_features]\n",
    "        \n",
    "        # Sample a random diffusion timestep for each sample in the batch\n",
    "        t = torch.randint(0, num_diffusion_steps, (batch_size,), dtype=torch.long)\n",
    "        \n",
    "        # Generate the noisy sample and the true noise using forward diffusion\n",
    "        x_t, true_noise = forward_diffusion_sample(x0, t, betas)\n",
    "        # print(f\"x_t shape : {x_t.shape}\")\n",
    "        \n",
    "        # Compute time embedding for the sampled timesteps\n",
    "        time_embedding = get_time_embedding(t, embedding_dim)\n",
    "        # print(f\"time_embedding shape : {time_embedding.shape}\")\n",
    "        \n",
    "        # Concatenate x_t with the time embedding to form the input to the denoising network\n",
    "        x_combined = torch.cat([x_t, time_embedding], dim=-1)\n",
    "        # x_combined shape is [batch_size, dim + embedding_dim]\n",
    "        # print(f\"x_combined shape : {x_combined.shape}\")\n",
    "        \n",
    "        # Get context embedding\n",
    "        context_embedding = context_net(context)\n",
    "        # print(context_embedding.shape)\n",
    "\n",
    "        x_combined = torch.cat([x_combined, context_embedding], dim=-1)\n",
    "        # print(f\"x_combined shape : {x_combined.shape}\")\n",
    "        \n",
    "        # Predict the noise using the denoising network directly\n",
    "        predicted_noise = denoise_net(x_combined)\n",
    "        \n",
    "        # Compute the loss between predicted noise and true noise\n",
    "        loss = F.mse_loss(predicted_noise, true_noise)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(denoise_net.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}, LR: {current_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 99\n",
      "timestep: 98\n",
      "timestep: 97\n",
      "timestep: 96\n",
      "timestep: 95\n",
      "timestep: 94\n",
      "timestep: 93\n",
      "timestep: 92\n",
      "timestep: 91\n",
      "timestep: 90\n",
      "timestep: 89\n",
      "timestep: 88\n",
      "timestep: 87\n",
      "timestep: 86\n",
      "timestep: 85\n",
      "timestep: 84\n",
      "timestep: 83\n",
      "timestep: 82\n",
      "timestep: 81\n",
      "timestep: 80\n",
      "timestep: 79\n",
      "timestep: 78\n",
      "timestep: 77\n",
      "timestep: 76\n",
      "timestep: 75\n",
      "timestep: 74\n",
      "timestep: 73\n",
      "timestep: 72\n",
      "timestep: 71\n",
      "timestep: 70\n",
      "timestep: 69\n",
      "timestep: 68\n",
      "timestep: 67\n",
      "timestep: 66\n",
      "timestep: 65\n",
      "timestep: 64\n",
      "timestep: 63\n",
      "timestep: 62\n",
      "timestep: 61\n",
      "timestep: 60\n",
      "timestep: 59\n",
      "timestep: 58\n",
      "timestep: 57\n",
      "timestep: 56\n",
      "timestep: 55\n",
      "timestep: 54\n",
      "timestep: 53\n",
      "timestep: 52\n",
      "timestep: 51\n",
      "timestep: 50\n",
      "timestep: 49\n",
      "timestep: 48\n",
      "timestep: 47\n",
      "timestep: 46\n",
      "timestep: 45\n",
      "timestep: 44\n",
      "timestep: 43\n",
      "timestep: 42\n",
      "timestep: 41\n",
      "timestep: 40\n",
      "timestep: 39\n",
      "timestep: 38\n",
      "timestep: 37\n",
      "timestep: 36\n",
      "timestep: 35\n",
      "timestep: 34\n",
      "timestep: 33\n",
      "timestep: 32\n",
      "timestep: 31\n",
      "timestep: 30\n",
      "timestep: 29\n",
      "timestep: 28\n",
      "timestep: 27\n",
      "timestep: 26\n",
      "timestep: 25\n",
      "timestep: 24\n",
      "timestep: 23\n",
      "timestep: 22\n",
      "timestep: 21\n",
      "timestep: 20\n",
      "timestep: 19\n",
      "timestep: 18\n",
      "timestep: 17\n",
      "timestep: 16\n",
      "timestep: 15\n",
      "timestep: 14\n",
      "timestep: 13\n",
      "timestep: 12\n",
      "timestep: 11\n",
      "timestep: 10\n",
      "timestep: 9\n",
      "timestep: 8\n",
      "timestep: 7\n",
      "timestep: 6\n",
      "timestep: 5\n",
      "timestep: 4\n",
      "timestep: 3\n",
      "timestep: 2\n",
      "timestep: 1\n",
      "timestep: 0\n",
      "Generated samples (predicted clean x0):\n",
      "tensor([[0.5941],\n",
      "        [0.0933],\n",
      "        [0.8901],\n",
      "        [0.9106],\n",
      "        [0.0569],\n",
      "        [0.0562],\n",
      "        [0.7168],\n",
      "        [0.2993],\n",
      "        [0.5847],\n",
      "        [0.2036],\n",
      "        [0.2590],\n",
      "        [0.9200],\n",
      "        [0.2415],\n",
      "        [0.1070],\n",
      "        [0.5739],\n",
      "        [0.5597]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "denoise_net.eval()\n",
    "context_net.eval()\n",
    "\n",
    "# Define hyperparameters \n",
    "num_diffusion_steps = 100  \n",
    "batch_size = 16            \n",
    "dim = 1                    \n",
    "\n",
    "# Run the reverse diffusion chain starting from noise\n",
    "generated_samples = run_reverse_diffusion(denoise_net, context_net,context, betas, num_diffusion_steps, batch_size, dim,embedding_dim)\n",
    "\n",
    "print(\"Generated samples (predicted clean x0):\")\n",
    "print(generated_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test MSE: 0.001140701769812343\n",
      "Batch 1 predictions:\n",
      " tensor([[0.7525],\n",
      "        [0.7534],\n",
      "        [0.7540],\n",
      "        [0.7558],\n",
      "        [0.7397],\n",
      "        [0.7399],\n",
      "        [0.7447],\n",
      "        [0.7499],\n",
      "        [0.7435],\n",
      "        [0.7431],\n",
      "        [0.7465],\n",
      "        [0.7456],\n",
      "        [0.7446],\n",
      "        [0.7463],\n",
      "        [0.7508],\n",
      "        [0.7613]])\n",
      "Batch 1 ground truth:\n",
      " tensor([[0.7483],\n",
      "        [0.7481],\n",
      "        [0.7619],\n",
      "        [0.7372],\n",
      "        [0.7394],\n",
      "        [0.7421],\n",
      "        [0.7533],\n",
      "        [0.7303],\n",
      "        [0.7416],\n",
      "        [0.7347],\n",
      "        [0.7410],\n",
      "        [0.7416],\n",
      "        [0.7525],\n",
      "        [0.7547],\n",
      "        [0.7805],\n",
      "        [0.7837]])\n",
      "Batch 2 predictions:\n",
      " tensor([[0.7664],\n",
      "        [0.7759],\n",
      "        [0.7814],\n",
      "        [0.7878],\n",
      "        [0.7941],\n",
      "        [0.7942],\n",
      "        [0.7897],\n",
      "        [0.7924],\n",
      "        [0.7924],\n",
      "        [0.7983],\n",
      "        [0.8027],\n",
      "        [0.7981],\n",
      "        [0.7911],\n",
      "        [0.7820],\n",
      "        [0.7753],\n",
      "        [0.7517]])\n",
      "Batch 2 ground truth:\n",
      " tensor([[0.7972],\n",
      "        [0.7925],\n",
      "        [0.8009],\n",
      "        [0.7999],\n",
      "        [0.7989],\n",
      "        [0.7943],\n",
      "        [0.7936],\n",
      "        [0.7876],\n",
      "        [0.8012],\n",
      "        [0.8090],\n",
      "        [0.7745],\n",
      "        [0.7601],\n",
      "        [0.7503],\n",
      "        [0.7480],\n",
      "        [0.7214],\n",
      "        [0.7227]])\n",
      "Batch 3 predictions:\n",
      " tensor([[0.7417],\n",
      "        [0.7221],\n",
      "        [0.7107],\n",
      "        [0.7091],\n",
      "        [0.7121],\n",
      "        [0.7148],\n",
      "        [0.6905],\n",
      "        [0.6912],\n",
      "        [0.6704],\n",
      "        [0.6697],\n",
      "        [0.6838],\n",
      "        [0.6911],\n",
      "        [0.7048],\n",
      "        [0.7169],\n",
      "        [0.7302],\n",
      "        [0.7320]])\n",
      "Batch 3 ground truth:\n",
      " tensor([[0.6999],\n",
      "        [0.6871],\n",
      "        [0.7074],\n",
      "        [0.7158],\n",
      "        [0.7135],\n",
      "        [0.6735],\n",
      "        [0.6818],\n",
      "        [0.6465],\n",
      "        [0.6647],\n",
      "        [0.7020],\n",
      "        [0.7015],\n",
      "        [0.7029],\n",
      "        [0.7245],\n",
      "        [0.7254],\n",
      "        [0.7180],\n",
      "        [0.7253]])\n",
      "Batch 4 predictions:\n",
      " tensor([[0.7317],\n",
      "        [0.7328],\n",
      "        [0.7331],\n",
      "        [0.7235],\n",
      "        [0.7125],\n",
      "        [0.7113],\n",
      "        [0.7017],\n",
      "        [0.7024],\n",
      "        [0.7092],\n",
      "        [0.7228],\n",
      "        [0.7426],\n",
      "        [0.7495],\n",
      "        [0.7652],\n",
      "        [0.7761],\n",
      "        [0.7813],\n",
      "        [0.7822]])\n",
      "Batch 4 ground truth:\n",
      " tensor([[0.7224],\n",
      "        [0.7210],\n",
      "        [0.7088],\n",
      "        [0.6955],\n",
      "        [0.7008],\n",
      "        [0.6906],\n",
      "        [0.7015],\n",
      "        [0.7155],\n",
      "        [0.7296],\n",
      "        [0.7533],\n",
      "        [0.7449],\n",
      "        [0.7693],\n",
      "        [0.7693],\n",
      "        [0.7775],\n",
      "        [0.7794],\n",
      "        [0.7881]])\n",
      "Batch 5 predictions:\n",
      " tensor([[0.7865],\n",
      "        [0.7929],\n",
      "        [0.7948],\n",
      "        [0.7928],\n",
      "        [0.7952],\n",
      "        [0.8026],\n",
      "        [0.7992],\n",
      "        [0.7992],\n",
      "        [0.7917],\n",
      "        [0.7891],\n",
      "        [0.7884],\n",
      "        [0.7897],\n",
      "        [0.7988],\n",
      "        [0.8104],\n",
      "        [0.8202],\n",
      "        [0.8291]])\n",
      "Batch 5 ground truth:\n",
      " tensor([[0.7921],\n",
      "        [0.7936],\n",
      "        [0.7836],\n",
      "        [0.7866],\n",
      "        [0.7961],\n",
      "        [0.7897],\n",
      "        [0.7893],\n",
      "        [0.7775],\n",
      "        [0.7818],\n",
      "        [0.7801],\n",
      "        [0.7877],\n",
      "        [0.8046],\n",
      "        [0.8185],\n",
      "        [0.8201],\n",
      "        [0.8284],\n",
      "        [0.8219]])\n",
      "Batch 6 predictions:\n",
      " tensor([[0.8329],\n",
      "        [0.8342],\n",
      "        [0.8387],\n",
      "        [0.8415],\n",
      "        [0.8424],\n",
      "        [0.8396],\n",
      "        [0.8400],\n",
      "        [0.8412],\n",
      "        [0.8473],\n",
      "        [0.8548],\n",
      "        [0.8575],\n",
      "        [0.8602],\n",
      "        [0.8606],\n",
      "        [0.8903],\n",
      "        [0.9116],\n",
      "        [0.9261]])\n",
      "Batch 6 ground truth:\n",
      " tensor([[0.8192],\n",
      "        [0.8316],\n",
      "        [0.8315],\n",
      "        [0.8288],\n",
      "        [0.8316],\n",
      "        [0.8327],\n",
      "        [0.8312],\n",
      "        [0.8560],\n",
      "        [0.8568],\n",
      "        [0.8569],\n",
      "        [0.8527],\n",
      "        [0.8491],\n",
      "        [0.9450],\n",
      "        [0.9421],\n",
      "        [0.9492],\n",
      "        [0.9649]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "denoise_net.eval()\n",
    "\n",
    "# Lists to store losses, predictions, and ground truths\n",
    "all_losses = []\n",
    "all_predictions = []\n",
    "all_ground_truths = []\n",
    "\n",
    "# Loop over the test DataLoader\n",
    "with torch.no_grad():\n",
    "    for context,x0 in test_loader:\n",
    "        # Ensure x0 is of shape [batch_size, dim]\n",
    "        if x0.dim() == 1:\n",
    "            x0 = x0.unsqueeze(1)\n",
    "        batch_size = x0.shape[0]\n",
    "\n",
    "        # Initialize x_T as pure Gaussian noise (same shape as x0)\n",
    "        x_T = torch.randn(batch_size, dim)\n",
    "\n",
    "        # Run the full reverse diffusion chain from t = num_diffusion_steps - 1 down to 0\n",
    "        x_t = x_T\n",
    "        for t_val in reversed(range(num_diffusion_steps)):\n",
    "            # Create a timestep tensor for the current reverse step (shape: [batch_size])\n",
    "            t_tensor = torch.full((batch_size,), t_val, dtype=torch.long)\n",
    "            # Perform one reverse diffusion step\n",
    "            # x_T, betas, timestep, embedding_dim, context, context_net, denoise_net\n",
    "            x_t = reverse_diffusion_sample(x_t, betas, t_tensor, embedding_dim,context,context_net, denoise_net)\n",
    "        \n",
    "        # The final x_t is the predicted clean sample x0_pred\n",
    "        x0_pred = x_t\n",
    "\n",
    "        # Compute the MSE loss between the predicted clean sample and the ground truth x0\n",
    "        loss = F.mse_loss(x0_pred, x0)\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        # Collect predictions and ground truth for further analysis (if needed)\n",
    "        all_predictions.append(x0_pred)\n",
    "        all_ground_truths.append(x0)\n",
    "\n",
    "# Compute the average test MSE over all batches\n",
    "average_loss = sum(all_losses) / len(all_losses)\n",
    "print(\"Average Test MSE:\", average_loss)\n",
    "\n",
    "# Inspect a few predictions vs. ground truth:\n",
    "for i in range(min(6, len(all_predictions))):\n",
    "    print(f\"Batch {i+1} predictions:\\n\", all_predictions[i])\n",
    "    print(f\"Batch {i+1} ground truth:\\n\", all_ground_truths[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
